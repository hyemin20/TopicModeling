{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference\n",
    "#https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eac5082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hyemi\\\\Python\\\\TopicModeling'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92531c84",
   "metadata": {},
   "source": [
    "# 1. method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/aphrodite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d472ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_bytes = page.read()\n",
    "html = html_bytes.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = html.find(\"<title>\")\n",
    "title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d562b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = title_index + len(\"<title>\")\n",
    "end_index = html.find(\"</title>\")\n",
    "title = html[start_index:end_index]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ccba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447846db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"ab*c\", \"AC\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f735953",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"a..c\", \"acac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806634e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"a.*c\", \"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_results = re.search(\"ab*c\", \"ACabc\", re.IGNORECASE)\n",
    "match_results.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Everything is [asdfa] if it's in <asfasdf>.\"\n",
    "string = re.sub(r\"<.*?>\", \"HI\", string)\n",
    "string = re.sub(r\"\\[.*?\\]\", \"HYEMIN\", string)\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97d95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "829e751f",
   "metadata": {},
   "source": [
    "# 2. method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd392636",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "title = re.sub(\"<.*?>\", \"\", title) # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Name: \"\n",
    "string_start_idx = html.find(string)\n",
    "text_start_idx = string_start_idx + len(string)\n",
    "\n",
    "next_html_tag_offset = html[text_start_idx:].find(\"<\")\n",
    "text_end_idx = text_start_idx + next_html_tag_offset\n",
    "\n",
    "raw_text = html[text_start_idx : text_end_idx]\n",
    "clean_text = raw_text.strip(\" \\r\\n\\t\") #시작과 끝 공백 제거\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f632d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Favorite Color: \"\n",
    "string_start_idx = html.find(string)\n",
    "text_start_idx = string_start_idx + len(string)\n",
    "\n",
    "next_html_tag_offset = html[text_start_idx:].find(\"<\")\n",
    "text_end_idx = text_start_idx + next_html_tag_offset\n",
    "\n",
    "raw_text = html[text_start_idx : text_end_idx]\n",
    "clean_text = raw_text.strip(\" \\r\\n\\t\") #시작과 끝 공백 제거\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4aeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in [\"Name: \", \"Favorite Color: \"]:\n",
    "    string_start_idx = html.find(string)\n",
    "    text_start_idx = string_start_idx + len(string)\n",
    "\n",
    "    next_html_tag_offset = html[text_start_idx:].find(\"<\")\n",
    "    text_end_idx = text_start_idx + next_html_tag_offset\n",
    "\n",
    "    raw_text = html[text_start_idx : text_end_idx]\n",
    "    clean_text = raw_text.strip(\" \\r\\n\\t\")\n",
    "    print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32436d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4dbd8e3",
   "metadata": {},
   "source": [
    "# 3. method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f995ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360941b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b434b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95956185",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, image2 = soup.find_all(\"img\")\n",
    "image1.name\n",
    "image1[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8575de",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fda11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "base_url = \"http://olympus.realpython.org\"\n",
    "page = urlopen(base_url + \"/profiles\")\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all(\"a\"):\n",
    "    link_url = base_url + link[\"href\"]\n",
    "    print(link_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cf057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6be86262",
   "metadata": {},
   "source": [
    "# 4. method 4 (Interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f763d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mechanicalsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanicalsoup\n",
    "browser = mechanicalsoup.Browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"https://www.tandfonline.com/loi/cedr20\"\n",
    "# \"https://www.linkedin.com/feed/\"\n",
    "url = \"https://www.jstor.org/stable/i262425\"\n",
    "page = browser.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d519f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = page.soup\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f661b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "form = html.select(\"form\")[0]\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "profiles_page = browser.submit(form, page.url)\n",
    "profiles_page.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd286be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = profiles_page.soup.select(\"a\")\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e7157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = \"http://olympus.realpython.org\"\n",
    "for link in links:\n",
    "    address = base_url + link[\"href\"]\n",
    "    text = link.text\n",
    "    print(f\"{text}: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44cc71",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 4.1 apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c37ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callibrary_url = \"https://auth.berkeley.edu/cas/login?service=https%3A%2F%2Fsearch.library.berkeley.edu%2Finfra%2FcasRedirect?ctx=/primaws\"\n",
    "callibrary_page = browser.get(callibrary_url)\n",
    "callibrary_page    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db036164",
   "metadata": {},
   "outputs": [],
   "source": [
    "callibrary = callibrary_page.soup\n",
    "callibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d867f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc7b2cfb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 4.2. example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/login\"\n",
    "page = browser.get(url)\n",
    "html = page.soup\n",
    "form = html.select(\"form\")[0]\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "profiles_page = browser.submit(form, page.url)\n",
    "profiles_page.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a736fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = profiles_page.soup.select(\"title\")\n",
    "texts = [link.text for link in links]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223afe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = profiles_page.soup\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa42595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eac5821",
   "metadata": {},
   "source": [
    "# 5. method 5 (Interact Real TIme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/dice\"\n",
    "page = browser.get(url)\n",
    "html = page.soup.select(\"#result\")[0]\n",
    "result = html.text\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I'm about to wait for five seconds...\")\n",
    "time.sleep(5)\n",
    "print(\"Done waiting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "for i in range(4):\n",
    "    page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "    tag = page.soup.select(\"#result\")[0]\n",
    "    result = tag.text\n",
    "    print(f\"The result of your dice roll is: {result}\")\n",
    "\n",
    "    # Wait 10 seconds if this isn't the last request\n",
    "    if i < 3:\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d5544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ec8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e007a03a",
   "metadata": {},
   "source": [
    "# 6. dataframe (frontier in psychology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdefa19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6164f5",
   "metadata": {},
   "source": [
    "##### Bring URL and HTML\n",
    "url = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3bcdb",
   "metadata": {},
   "source": [
    "##### Convert BeautifulSoup object to string\n",
    "html_content = str(soup)\n",
    "\n",
    "##### Search for title pattern\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html_content, re.IGNORECASE)\n",
    "title = match_results.group() if match_results else \"No title found\"\n",
    "title = re.sub(\"<.*?>\", \"\", title)  # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e360e6",
   "metadata": {},
   "source": [
    "##### https://www.frontiersin.org/journals/psychology/articles?publication-date=25%2F10%2F2024-25%2F10%2F2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6af844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles loaded: 32, Retries: 0\n",
      "Articles loaded: 48, Retries: 0\n",
      "Articles loaded: 64, Retries: 0\n",
      "Articles loaded: 80, Retries: 0\n",
      "Articles loaded: 96, Retries: 0\n",
      "Articles loaded: 112, Retries: 0\n",
      "Articles loaded: 128, Retries: 0\n",
      "Articles loaded: 144, Retries: 0\n",
      "Articles loaded: 160, Retries: 0\n",
      "Articles loaded: 176, Retries: 0\n",
      "Articles loaded: 192, Retries: 0\n",
      "Articles loaded: 208, Retries: 0\n",
      "Articles loaded: 224, Retries: 0\n",
      "Articles loaded: 240, Retries: 0\n",
      "Articles loaded: 256, Retries: 0\n",
      "Articles loaded: 272, Retries: 0\n",
      "Articles loaded: 288, Retries: 0\n",
      "Articles loaded: 304, Retries: 0\n",
      "Articles loaded: 320, Retries: 0\n",
      "Articles loaded: 336, Retries: 0\n",
      "Articles loaded: 368, Retries: 0\n",
      "Articles loaded: 384, Retries: 0\n",
      "Articles loaded: 400, Retries: 0\n",
      "Articles loaded: 416, Retries: 0\n",
      "Articles loaded: 432, Retries: 0\n",
      "Articles loaded: 448, Retries: 0\n",
      "Articles loaded: 464, Retries: 0\n",
      "Articles loaded: 480, Retries: 0\n",
      "Articles loaded: 496, Retries: 0\n",
      "Articles loaded: 512, Retries: 0\n",
      "Articles loaded: 528, Retries: 0\n",
      "Articles loaded: 544, Retries: 0\n",
      "Articles loaded: 560, Retries: 0\n",
      "Articles loaded: 576, Retries: 0\n",
      "Articles loaded: 592, Retries: 0\n",
      "Articles loaded: 608, Retries: 0\n",
      "Articles loaded: 624, Retries: 0\n",
      "Articles loaded: 640, Retries: 0\n",
      "Articles loaded: 656, Retries: 0\n",
      "Articles loaded: 672, Retries: 0\n",
      "Articles loaded: 688, Retries: 0\n",
      "Articles loaded: 704, Retries: 0\n",
      "Articles loaded: 720, Retries: 0\n",
      "Articles loaded: 736, Retries: 0\n",
      "Articles loaded: 752, Retries: 0\n",
      "Articles loaded: 768, Retries: 0\n",
      "Articles loaded: 784, Retries: 0\n",
      "Articles loaded: 800, Retries: 0\n",
      "Articles loaded: 816, Retries: 0\n",
      "Articles loaded: 832, Retries: 0\n",
      "Articles loaded: 848, Retries: 0\n",
      "Articles loaded: 864, Retries: 0\n",
      "Articles loaded: 880, Retries: 0\n",
      "Articles loaded: 896, Retries: 0\n",
      "Articles loaded: 912, Retries: 0\n",
      "Articles loaded: 928, Retries: 0\n",
      "Articles loaded: 944, Retries: 0\n",
      "Articles loaded: 960, Retries: 0\n",
      "Articles loaded: 976, Retries: 0\n",
      "Articles loaded: 992, Retries: 0\n",
      "Articles loaded: 1008, Retries: 0\n",
      "Articles loaded: 1008, Retries: 1\n",
      "Articles loaded: 1008, Retries: 2\n",
      "Articles loaded: 1008, Retries: 3\n",
      "Articles loaded: 1008, Retries: 4\n",
      "Articles loaded: 1008, Retries: 5\n",
      "Articles loaded: 1008, Retries: 6\n",
      "Articles loaded: 1008, Retries: 7\n",
      "Articles loaded: 1008, Retries: 8\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[1;32m---> 65\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCardArticle__title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m     publish_date \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCardArticle__date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m     article_type \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCardArticle__type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the page\n",
    "base_url = \"https://www.frontiersin.org/journals/psychology/articles\"\n",
    "url = base_url + \"?publication-date=01%2F01%2F2014-31%2F12%2F2014\"\n",
    "driver.get(url)\n",
    "\n",
    "# Set page zoom to 25%\n",
    "driver.execute_script(\"document.body.style.zoom='25%'\")\n",
    "\n",
    "# Accept cookies if the banner appears\n",
    "try:\n",
    "    accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "    )\n",
    "    accept_cookies_button.click()\n",
    "except:\n",
    "    print(\"No cookie banner detected.\")\n",
    "\n",
    "# Wait for the first article to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"CardArticle\"))\n",
    ")\n",
    "\n",
    "# Scroll and hover to trigger loading of new articles\n",
    "scroll_increment = 200  # Smaller increment for smoother scroll\n",
    "scroll_pause_time = 1.5 # Adjusted pause time to simulate user interaction\n",
    "max_retries = 8         # Retry limit if no new articles load\n",
    "retries = 0\n",
    "prev_article_count = 0\n",
    "\n",
    "# Scroll until all articles are loaded\n",
    "actions = ActionChains(driver)\n",
    "while retries < max_retries:\n",
    "    # Scroll by a small increment\n",
    "    driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "    time.sleep(scroll_pause_time)\n",
    "\n",
    "    # Hover over the footer to trigger lazy loading if any exists\n",
    "    footer = driver.find_element(By.TAG_NAME, \"footer\")\n",
    "    actions.move_to_element(footer).perform()\n",
    "\n",
    "    # Check current number of articles loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all(\"article\", class_=\"CardArticle\")\n",
    "    current_article_count = len(articles)\n",
    "\n",
    "    # Update retry count based on loaded articles\n",
    "    if current_article_count > prev_article_count:\n",
    "        prev_article_count = current_article_count\n",
    "        retries = 0  # Reset retries if new articles loaded\n",
    "    else:\n",
    "        retries += 1  # Increment retry count if no new articles load\n",
    "\n",
    "    print(f\"Articles loaded: {current_article_count}, Retries: {retries}\")\n",
    "\n",
    "# Ensure elements are fully loaded\n",
    "time.sleep(3)\n",
    "\n",
    "# Extract article information\n",
    "data = []\n",
    "for article in articles:\n",
    "    title = article.find(\"h1\", class_=\"CardArticle__title\").get_text(strip=True)\n",
    "    publish_date = article.find(\"p\", class_=\"CardArticle__date\").get_text(strip=True)\n",
    "    article_type = article.find(\"p\", class_=\"CardArticle__type\").get_text(strip=True)\n",
    "    link = article.find(\"a\", class_=\"CardArticle__wrapper\")[\"href\"]\n",
    "        \n",
    "    data.append({\n",
    "        \"Title\": title,\n",
    "        \"Type\": article_type,\n",
    "        \"Published Date\": publish_date,\n",
    "        \"Link\": link,\n",
    "    })\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df = pd.DataFrame(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5084118b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Type, Published Date, Link]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.iloc[829:834]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "159e75a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inter-trial priming does not affect attentiona...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rapid heartbeat, but dry palms: reactions of h...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Limits to the usability of iconic memory</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inter-synaptic learning of combination rules i...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cafeteria diet impairs expression of sensory-s...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>Neurophysiological evidence that musical train...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>A greater decline in female facial attractiven...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>Within, but not between hands interactions in ...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Cut! that’s a wrap: regulating negative emotio...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>Supervised classification in the presence of m...</td>\n",
       "      <td>Original Research</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title               Type  \\\n",
       "0    Inter-trial priming does not affect attentiona...  Original Research   \n",
       "1    Rapid heartbeat, but dry palms: reactions of h...  Original Research   \n",
       "3             Limits to the usability of iconic memory  Original Research   \n",
       "5    Inter-synaptic learning of combination rules i...  Original Research   \n",
       "6    Cafeteria diet impairs expression of sensory-s...  Original Research   \n",
       "..                                                 ...                ...   \n",
       "784  Neurophysiological evidence that musical train...  Original Research   \n",
       "788  A greater decline in female facial attractiven...  Original Research   \n",
       "789  Within, but not between hands interactions in ...  Original Research   \n",
       "790  Cut! that’s a wrap: regulating negative emotio...  Original Research   \n",
       "791  Supervised classification in the presence of m...  Original Research   \n",
       "\n",
       "    Published Date                                               Link  \n",
       "0             2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "1             2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "3             2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "5             2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "6             2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "..             ...                                                ...  \n",
       "784           2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "788           2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "789           2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "790           2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "791           2014  https://www.frontiersin.org/journals/psycholog...  \n",
       "\n",
       "[471 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Published Date'] = pd.to_datetime(\n",
    "    df['Published Date'].str.replace(r'(Published on |Accepted on )', '', regex=True),\n",
    "    dayfirst=True,\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "df['Published Date'] = df['Published Date'].dt.strftime('%Y')\n",
    "df['Published Date'] = df['Published Date'].fillna('2014')\n",
    "\n",
    "# Filter for 'Original Research' type\n",
    "DF = df[df['Type'] == 'Original Research']\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7932307c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeac4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda5c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1435688/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#paragraphs = soup.find_all('p')\n",
    "#paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390d8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7934f400-6f6b-4aec-83c0-99a6d34c2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles loaded: 32, Retries: 0\n",
      "Articles loaded: 48, Retries: 0\n",
      "Articles loaded: 64, Retries: 0\n",
      "Articles loaded: 80, Retries: 0\n",
      "Articles loaded: 96, Retries: 0\n",
      "Articles loaded: 112, Retries: 0\n",
      "Articles loaded: 128, Retries: 0\n",
      "Articles loaded: 144, Retries: 0\n",
      "Articles loaded: 160, Retries: 0\n",
      "Articles loaded: 176, Retries: 0\n",
      "Articles loaded: 192, Retries: 0\n",
      "Articles loaded: 208, Retries: 0\n",
      "Articles loaded: 240, Retries: 0\n",
      "Articles loaded: 240, Retries: 1\n",
      "Articles loaded: 256, Retries: 0\n",
      "Articles loaded: 272, Retries: 0\n",
      "Articles loaded: 288, Retries: 0\n",
      "Articles loaded: 304, Retries: 0\n",
      "Articles loaded: 320, Retries: 0\n",
      "Articles loaded: 336, Retries: 0\n",
      "Articles loaded: 352, Retries: 0\n",
      "Articles loaded: 368, Retries: 0\n",
      "Articles loaded: 384, Retries: 0\n",
      "Articles loaded: 400, Retries: 0\n",
      "Articles loaded: 416, Retries: 0\n",
      "Articles loaded: 432, Retries: 0\n",
      "Articles loaded: 448, Retries: 0\n",
      "Articles loaded: 464, Retries: 0\n",
      "Articles loaded: 480, Retries: 0\n",
      "Articles loaded: 496, Retries: 0\n",
      "Articles loaded: 512, Retries: 0\n",
      "Articles loaded: 528, Retries: 0\n",
      "Articles loaded: 544, Retries: 0\n",
      "Articles loaded: 560, Retries: 0\n",
      "Articles loaded: 576, Retries: 0\n",
      "Articles loaded: 592, Retries: 0\n",
      "Articles loaded: 608, Retries: 0\n",
      "Articles loaded: 624, Retries: 0\n",
      "Articles loaded: 640, Retries: 0\n",
      "Articles loaded: 656, Retries: 0\n",
      "Articles loaded: 672, Retries: 0\n",
      "Articles loaded: 688, Retries: 0\n",
      "Articles loaded: 704, Retries: 0\n",
      "Articles loaded: 720, Retries: 0\n",
      "Articles loaded: 736, Retries: 0\n",
      "Articles loaded: 752, Retries: 0\n",
      "Articles loaded: 768, Retries: 0\n",
      "Articles loaded: 784, Retries: 0\n",
      "Articles loaded: 800, Retries: 0\n",
      "Articles loaded: 816, Retries: 0\n",
      "Articles loaded: 832, Retries: 0\n",
      "Articles loaded: 848, Retries: 0\n",
      "Articles loaded: 864, Retries: 0\n",
      "Articles loaded: 880, Retries: 0\n",
      "Articles loaded: 896, Retries: 0\n",
      "Articles loaded: 912, Retries: 0\n",
      "Articles loaded: 928, Retries: 0\n",
      "Articles loaded: 944, Retries: 0\n",
      "Articles loaded: 960, Retries: 0\n",
      "Articles loaded: 976, Retries: 0\n",
      "Articles loaded: 992, Retries: 0\n",
      "Articles loaded: 1008, Retries: 0\n",
      "Articles loaded: 1008, Retries: 1\n",
      "Articles loaded: 1008, Retries: 2\n",
      "Articles loaded: 1008, Retries: 3\n",
      "Articles loaded: 1008, Retries: 4\n",
      "Articles loaded: 1008, Retries: 5\n",
      "Articles loaded: 1008, Retries: 6\n",
      "Articles loaded: 1008, Retries: 7\n",
      "Articles loaded: 1008, Retries: 8\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1008 entries, 0 to 1007\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Title           0 non-null      object\n",
      " 1   Type            1008 non-null   object\n",
      " 2   Published Date  1008 non-null   object\n",
      " 3   Link            1008 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.6+ KB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "  Title               Type            Published Date  \\\n",
      "0  None  Original Research  Published on 23 Dec 2014   \n",
      "1  None            Methods  Published on 23 Dec 2014   \n",
      "2  None  Original Research  Published on 23 Dec 2014   \n",
      "3  None          Editorial  Published on 23 Dec 2014   \n",
      "4  None  Original Research  Published on 23 Dec 2014   \n",
      "\n",
      "                                                Link  \n",
      "0  https://www.frontiersin.org/journals/psycholog...  \n",
      "1  https://www.frontiersin.org/journals/psycholog...  \n",
      "2  https://www.frontiersin.org/journals/psycholog...  \n",
      "3  https://www.frontiersin.org/journals/psycholog...  \n",
      "4  https://www.frontiersin.org/journals/psycholog...  \n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Open the page\n",
    "base_url = \"https://www.frontiersin.org/journals/psychology/articles\"\n",
    "url = base_url + \"?publication-date=01%2F01%2F2014-31%2F12%2F2014\"\n",
    "driver.get(url)\n",
    "\n",
    "# Set page zoom to 25%\n",
    "driver.execute_script(\"document.body.style.zoom='25%'\")\n",
    "\n",
    "# Accept cookies if the banner appears\n",
    "try:\n",
    "    accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "    )\n",
    "    accept_cookies_button.click()\n",
    "except:\n",
    "    print(\"No cookie banner detected.\")\n",
    "\n",
    "# Wait for the first article to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"CardArticle\"))\n",
    ")\n",
    "\n",
    "# Scroll and hover to trigger loading of new articles\n",
    "scroll_increment = 200\n",
    "scroll_pause_time = 1.5\n",
    "max_retries = 8\n",
    "retries = 0\n",
    "prev_article_count = 0\n",
    "\n",
    "# Scroll until all articles are loaded\n",
    "actions = ActionChains(driver)\n",
    "while retries < max_retries:\n",
    "    # Scroll by a small increment\n",
    "    driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "    time.sleep(scroll_pause_time)\n",
    "    \n",
    "    # Hover over the footer to trigger lazy loading\n",
    "    footer = driver.find_element(By.TAG_NAME, \"footer\")\n",
    "    actions.move_to_element(footer).perform()\n",
    "    \n",
    "    # Check current number of articles loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all(\"article\", class_=\"CardArticle\")\n",
    "    current_article_count = len(articles)\n",
    "    \n",
    "    # Update retry count based on loaded articles\n",
    "    if current_article_count > prev_article_count:\n",
    "        prev_article_count = current_article_count\n",
    "        retries = 0  # Reset retries if new articles loaded\n",
    "    else:\n",
    "        retries += 1  # Increment retry count if no new articles load\n",
    "    print(f\"Articles loaded: {current_article_count}, Retries: {retries}\")\n",
    "\n",
    "# Ensure elements are fully loaded\n",
    "time.sleep(3)\n",
    "\n",
    "# Extract article information with error handling\n",
    "data = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        # Find elements with None handling\n",
    "        title_elem = article.find(\"h1\", class_=\"CardArticle__title\")\n",
    "        date_elem = article.find(\"p\", class_=\"CardArticle__date\")\n",
    "        type_elem = article.find(\"p\", class_=\"CardArticle__type\")\n",
    "        link_elem = article.find(\"a\", class_=\"CardArticle__wrapper\")\n",
    "        \n",
    "        # Extract data with None checks\n",
    "        title = title_elem.get_text(strip=True) if title_elem else None\n",
    "        publish_date = date_elem.get_text(strip=True) if date_elem else None\n",
    "        article_type = type_elem.get_text(strip=True) if type_elem else None\n",
    "        link = link_elem[\"href\"] if link_elem else None\n",
    "        \n",
    "        data.append({\n",
    "            \"Title\": title,\n",
    "            \"Type\": article_type,\n",
    "            \"Published Date\": publish_date,\n",
    "            \"Link\": link,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display first 5 rows and basic info about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df.to_csv('frontiers_psychology_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ded804-7b5d-4d89-b2ab-38e0b2d43322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and process the data\n",
    "def clean_publication_dates(df):\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Clean date strings and convert to datetime\n",
    "    df['Published Date'] = df['Published Date'].str.replace(\n",
    "        r'(Published on |Accepted on )', '', \n",
    "        regex=True\n",
    "    )\n",
    "    \n",
    "    # Convert to datetime with proper error handling\n",
    "    df['Published Date'] = pd.to_datetime(\n",
    "        df['Published Date'],\n",
    "        dayfirst=True,  # Assuming date format is DD/MM/YYYY\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Extract year and handle missing values\n",
    "    df['Year'] = df['Published Date'].dt.strftime('%Y')\n",
    "    df['Year'] = df['Year'].fillna('2014')  # Fill missing years with 2014\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_research_articles(df):\n",
    "    # Filter for Original Research articles\n",
    "    research_df = df[df['Type'] == 'Original Research'].copy()\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    research_df = research_df.reset_index(drop=True)\n",
    "    \n",
    "    return research_df\n",
    "\n",
    "# Process the dataframe\n",
    "processed_df = clean_publication_dates(df)\n",
    "\n",
    "# Filter for Original Research articles\n",
    "DF = filter_research_articles(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d778575a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='www.frontiersin.org', port=443): Max retries exceeded with url: /journals/psychology/articles/10.3389/fpsyg.2014.00966/full (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029988F09400>, 'Connection to www.frontiersin.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    703\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:207\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    210\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x0000029988F09400>, 'Connection to www.frontiersin.org timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.frontiersin.org', port=443): Max retries exceeded with url: /journals/psychology/articles/10.3389/fpsyg.2014.00966/full (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029988F09400>, 'Connection to www.frontiersin.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m DF\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      8\u001b[0m     link \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-Agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMozilla/5.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Title\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='www.frontiersin.org', port=443): Max retries exceeded with url: /journals/psychology/articles/10.3389/fpsyg.2014.00966/full (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000029988F09400>, 'Connection to www.frontiersin.org timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "# 각 링크에서 author와 abstract, full text 추출\n",
    "sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "\n",
    "# Iterate through each link in the DataFrame to extract data\n",
    "for index, row in DF.iterrows():\n",
    "    link = row['Link']\n",
    "    response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Title\n",
    "    title = row['Title']\n",
    "    \n",
    "    # Authors\n",
    "    authors = [author.get_text(strip=True) for author in soup.select('.authors .author-wrapper')]\n",
    "    \n",
    "    # Abstract extraction using <meta> tag content\n",
    "    abstract = None\n",
    "    try:\n",
    "        abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "        if abstract_meta:\n",
    "            abstract_content = abstract_meta['content']\n",
    "            abstract = re.search(r'<p>(.*?)</p>', abstract_content).group(1) if '<p>' in abstract_content else abstract_content\n",
    "        else:\n",
    "            abstract = \"Abstract not available.\"\n",
    "    except AttributeError:\n",
    "        abstract = \"Abstract not available.\"\n",
    "    \n",
    "    # Full Text extraction until the first target section\n",
    "    #extracted_text = []\n",
    "    #for paragraph in soup.find_all(['p', 'h2']):\n",
    "    #    # If we encounter any target section, stop the extraction\n",
    "    #    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "    #        break\n",
    "    #    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "    #        extracted_text.append(paragraph.text.strip())\n",
    "    \n",
    "    # Join all extracted paragraphs into one full text string\n",
    "    #full_text = \" \".join(extracted_text)\n",
    "    \n",
    "    # Add data to the list\n",
    "    data.append({\n",
    "        \"Title\": title,\n",
    "        \"Published Date\": row['Published Date'],\n",
    "        \"Type\": row['Type'],\n",
    "        \"Link\": link,\n",
    "        \"Authors\": \", \".join(authors),\n",
    "        \"Abstract\": abstract,\n",
    "        #\"Full Text\": full_text\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "result_df = pd.DataFrame(data)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Title'] = result_df['Title'].apply(lambda x: re.sub(r'(\\d+|\\*|†)', '', re.sub(r'\\(.*?\\)', '', x)))\n",
    "result_df['Abstract'] = result_df['Abstract'].apply(lambda x: re.sub(r'(\\d+|\\*|†)', '', re.sub(r'\\(.*?\\)', '', x)))\n",
    "result_df['Abstract'] = result_df['Abstract'].apply(lambda x: re.sub(r'<\\d+|\\*|†>', '', re.sub(r'\\<.*?\\>', '', x)))\n",
    "#result_df['Full Text'] = result_df['Full Text'].apply(lambda x: re.sub(r'(\\d+|\\*|†)', '', re.sub(r'\\(.*?\\)', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Authors'] = result_df['Authors'].apply(\n",
    "    lambda x: re.sub(r'(\\d+|\\*|†)', '', x)  # Remove digits, asterisks, and dagger symbols\n",
    "                .replace(\"??\", \"\")          # Remove any question marks\n",
    "                .replace(\",,\", \",\")         # Consolidate multiple commas\n",
    "                .replace(\", ,\", \",\")        # Remove spaced commas\n",
    "                .replace(\"  \", \" \")         # Remove double spaces\n",
    "                .replace(\", ,\", \",\")                # Remove any trailing commas or spaces\n",
    ")\n",
    "print(result_df[['Title', 'Authors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1da9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"articles_data_14.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5437233",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5437fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
