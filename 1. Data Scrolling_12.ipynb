{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eac5082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hyemi\\\\Python\\\\TopicModeling'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007a03a",
   "metadata": {},
   "source": [
    "# Frontier in Psychology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712807f3-b4f8-481c-a49d-0279316e9353",
   "metadata": {},
   "source": [
    "# 1.background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdefa19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6164f5",
   "metadata": {},
   "source": [
    "##### Bring URL and HTML\n",
    "url = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3bcdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Convert BeautifulSoup object to string\n",
    "html_content = str(soup)\n",
    "\n",
    "##### Search for title pattern\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html_content, re.IGNORECASE)\n",
    "title = match_results.group() if match_results else \"No title found\"\n",
    "title = re.sub(\"<.*?>\", \"\", title)  # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e360e6",
   "metadata": {},
   "source": [
    "##### https://www.frontiersin.org/journals/psychology/articles?publication-date=25%2F10%2F2024-25%2F10%2F2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6af844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles loaded: 32, Retries: 0\n",
      "Articles loaded: 48, Retries: 0\n",
      "Articles loaded: 64, Retries: 0\n",
      "Articles loaded: 80, Retries: 0\n",
      "Articles loaded: 96, Retries: 0\n",
      "Articles loaded: 112, Retries: 0\n",
      "Articles loaded: 128, Retries: 0\n",
      "Articles loaded: 144, Retries: 0\n",
      "Articles loaded: 160, Retries: 0\n",
      "Articles loaded: 176, Retries: 0\n",
      "Articles loaded: 192, Retries: 0\n",
      "Articles loaded: 208, Retries: 0\n",
      "Articles loaded: 224, Retries: 0\n",
      "Articles loaded: 240, Retries: 0\n",
      "Articles loaded: 256, Retries: 0\n",
      "Articles loaded: 272, Retries: 0\n",
      "Articles loaded: 288, Retries: 0\n",
      "Articles loaded: 304, Retries: 0\n",
      "Articles loaded: 320, Retries: 0\n",
      "Articles loaded: 336, Retries: 0\n",
      "Articles loaded: 352, Retries: 0\n",
      "Articles loaded: 368, Retries: 0\n",
      "Articles loaded: 384, Retries: 0\n",
      "Articles loaded: 400, Retries: 0\n",
      "Articles loaded: 416, Retries: 0\n",
      "Articles loaded: 432, Retries: 0\n",
      "Articles loaded: 448, Retries: 0\n",
      "Articles loaded: 464, Retries: 0\n",
      "Articles loaded: 480, Retries: 0\n",
      "Articles loaded: 496, Retries: 0\n",
      "Articles loaded: 512, Retries: 0\n",
      "Articles loaded: 528, Retries: 0\n",
      "Articles loaded: 544, Retries: 0\n",
      "Articles loaded: 560, Retries: 0\n",
      "Articles loaded: 576, Retries: 0\n",
      "Articles loaded: 584, Retries: 0\n",
      "Articles loaded: 584, Retries: 1\n",
      "Articles loaded: 584, Retries: 2\n",
      "Articles loaded: 584, Retries: 3\n",
      "Articles loaded: 584, Retries: 4\n",
      "Articles loaded: 584, Retries: 5\n",
      "Articles loaded: 584, Retries: 6\n",
      "Articles loaded: 584, Retries: 7\n",
      "Articles loaded: 584, Retries: 8\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Open the page\n",
    "base_url = \"https://www.frontiersin.org/journals/psychology/articles\"\n",
    "url = base_url + \"?publication-date=01%2F01%2F2012-31%2F12%2F2012\"\n",
    "driver.get(url)\n",
    "\n",
    "# Set page zoom to 25%\n",
    "driver.execute_script(\"document.body.style.zoom='25%'\")\n",
    "\n",
    "# Accept cookies if the banner appears\n",
    "try:\n",
    "    accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "    )\n",
    "    accept_cookies_button.click()\n",
    "except:\n",
    "    print(\"No cookie banner detected.\")\n",
    "\n",
    "# Wait for the first article to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"CardArticle\"))\n",
    ")\n",
    "\n",
    "# Scroll and hover to trigger loading of new articles\n",
    "scroll_increment = 200\n",
    "scroll_pause_time = 1.5\n",
    "max_retries = 8\n",
    "retries = 0\n",
    "prev_article_count = 0\n",
    "\n",
    "# Scroll until all articles are loaded\n",
    "actions = ActionChains(driver)\n",
    "while retries < max_retries:\n",
    "    # Scroll by a small increment\n",
    "    driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "    time.sleep(scroll_pause_time)\n",
    "    \n",
    "    # Hover over the footer to trigger lazy loading\n",
    "    footer = driver.find_element(By.TAG_NAME, \"footer\")\n",
    "    actions.move_to_element(footer).perform()\n",
    "    \n",
    "    # Check current number of articles loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all(\"article\", class_=\"CardArticle\")\n",
    "    current_article_count = len(articles)\n",
    "    \n",
    "    # Update retry count based on loaded articles\n",
    "    if current_article_count > prev_article_count:\n",
    "        prev_article_count = current_article_count\n",
    "        retries = 0  # Reset retries if new articles loaded\n",
    "    else:\n",
    "        retries += 1  # Increment retry count if no new articles load\n",
    "    print(f\"Articles loaded: {current_article_count}, Retries: {retries}\")\n",
    "\n",
    "# Ensure elements are fully loaded\n",
    "time.sleep(3)\n",
    "\n",
    "# Extract article information with error handling\n",
    "data = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        # Find elements with None handling\n",
    "        title_elem = article.find(\"h1\", class_=\"CardArticle__title\")\n",
    "        date_elem = article.find(\"p\", class_=\"CardArticle__date\")\n",
    "        type_elem = article.find(\"p\", class_=\"CardArticle__type\")\n",
    "        link_elem = article.find(\"a\", class_=\"CardArticle__wrapper\")\n",
    "        \n",
    "        # Extract data with None checks\n",
    "        title = title_elem.get_text(strip=True) if title_elem else None\n",
    "        publish_date = date_elem.get_text(strip=True) if date_elem else None\n",
    "        article_type = type_elem.get_text(strip=True) if type_elem else None\n",
    "        link = link_elem[\"href\"] if link_elem else None\n",
    "        \n",
    "        data.append({\n",
    "            \"Title\": title,\n",
    "            \"Type\": article_type,\n",
    "            \"Published Date\": publish_date,\n",
    "            \"Link\": link,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159e75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and process the data\n",
    "def clean_publication_dates(df):\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Clean date strings and convert to datetime\n",
    "    df['Published Date'] = df['Published Date'].str.replace(\n",
    "        r'(Published on |Accepted on )', '', \n",
    "        regex=True\n",
    "    )\n",
    "    \n",
    "    # Convert to datetime with proper error handling\n",
    "    df['Published Date'] = pd.to_datetime(\n",
    "        df['Published Date'],\n",
    "        dayfirst=True,  # Assuming date format is DD/MM/YYYY\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Extract year and handle missing values\n",
    "    df['Year'] = df['Published Date'].dt.strftime('%Y')\n",
    "    df['Year'] = df['Year'].fillna('2012')  # Fill missing years with 2014\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_research_articles(df):\n",
    "    # Filter for Original Research articles\n",
    "    research_df = df[df['Type'] == 'Original Research'].copy()\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    research_df = research_df.reset_index(drop=True)\n",
    "    \n",
    "    return research_df\n",
    "\n",
    "# Process the dataframe\n",
    "processed_df = clean_publication_dates(df)\n",
    "\n",
    "# Filter for Original Research articles\n",
    "research_df = filter_research_articles(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7932307c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dfeac4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da64b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0bda5c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1435688/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#paragraphs = soup.find_all('p')\n",
    "#paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2390d8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1994fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce65f4a-4f71-466b-8c12-4ea6a19a7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 1/378\n",
      "Processing article 2/378\n",
      "Processing article 3/378\n",
      "Processing article 4/378\n",
      "Processing article 5/378\n",
      "Processing article 6/378\n",
      "Processing article 7/378\n",
      "Processing article 8/378\n",
      "Processing article 9/378\n",
      "Processing article 10/378\n",
      "Processing article 11/378\n",
      "Processing article 12/378\n",
      "Processing article 13/378\n",
      "Processing article 14/378\n",
      "Processing article 15/378\n",
      "Processing article 16/378\n",
      "Processing article 17/378\n",
      "Processing article 18/378\n",
      "Processing article 19/378\n",
      "Processing article 20/378\n",
      "Processing article 21/378\n",
      "Processing article 22/378\n",
      "Processing article 23/378\n",
      "Processing article 24/378\n",
      "Processing article 25/378\n",
      "Processing article 26/378\n",
      "Processing article 27/378\n",
      "Processing article 28/378\n",
      "Processing article 29/378\n",
      "Processing article 30/378\n",
      "Processing article 31/378\n",
      "Processing article 32/378\n",
      "Processing article 33/378\n",
      "Processing article 34/378\n",
      "Processing article 35/378\n",
      "Processing article 36/378\n",
      "Processing article 37/378\n",
      "Processing article 38/378\n",
      "Processing article 39/378\n",
      "Processing article 40/378\n",
      "Processing article 41/378\n",
      "Processing article 42/378\n",
      "Processing article 43/378\n",
      "Processing article 44/378\n",
      "Processing article 45/378\n",
      "Processing article 46/378\n",
      "Processing article 47/378\n",
      "Processing article 48/378\n",
      "Processing article 49/378\n",
      "Processing article 50/378\n",
      "Processing article 51/378\n",
      "Processing article 52/378\n",
      "Processing article 53/378\n",
      "Processing article 54/378\n",
      "Processing article 55/378\n",
      "Processing article 56/378\n",
      "Processing article 57/378\n",
      "Processing article 58/378\n",
      "Processing article 59/378\n",
      "Processing article 60/378\n",
      "Processing article 61/378\n",
      "Processing article 62/378\n",
      "Processing article 63/378\n",
      "Processing article 64/378\n",
      "Processing article 65/378\n",
      "Processing article 66/378\n",
      "Processing article 67/378\n",
      "Processing article 68/378\n",
      "Processing article 69/378\n",
      "Processing article 70/378\n",
      "Processing article 71/378\n",
      "Processing article 72/378\n",
      "Processing article 73/378\n",
      "Processing article 74/378\n",
      "Processing article 75/378\n",
      "Processing article 76/378\n",
      "Processing article 77/378\n",
      "Processing article 78/378\n",
      "Processing article 79/378\n",
      "Processing article 80/378\n",
      "Processing article 81/378\n",
      "Processing article 82/378\n",
      "Processing article 83/378\n",
      "Processing article 84/378\n",
      "Processing article 85/378\n",
      "Processing article 86/378\n",
      "Processing article 87/378\n",
      "Processing article 88/378\n",
      "Processing article 89/378\n",
      "Processing article 90/378\n",
      "Processing article 91/378\n",
      "Processing article 92/378\n",
      "Processing article 93/378\n",
      "Processing article 94/378\n",
      "Processing article 95/378\n",
      "Processing article 96/378\n",
      "Processing article 97/378\n",
      "Processing article 98/378\n",
      "Processing article 99/378\n",
      "Processing article 100/378\n",
      "Processing article 101/378\n",
      "Processing article 102/378\n",
      "Processing article 103/378\n",
      "Processing article 104/378\n",
      "Processing article 105/378\n",
      "Processing article 106/378\n",
      "Processing article 107/378\n",
      "Processing article 108/378\n",
      "Processing article 109/378\n",
      "Processing article 110/378\n",
      "Processing article 111/378\n",
      "Processing article 112/378\n",
      "Processing article 113/378\n",
      "Processing article 114/378\n",
      "Processing article 115/378\n",
      "Processing article 116/378\n",
      "Processing article 117/378\n",
      "Processing article 118/378\n",
      "Processing article 119/378\n",
      "Processing article 120/378\n",
      "Processing article 121/378\n",
      "Processing article 122/378\n",
      "Processing article 123/378\n",
      "Processing article 124/378\n",
      "Processing article 125/378\n",
      "Processing article 126/378\n",
      "Processing article 127/378\n",
      "Processing article 128/378\n",
      "Processing article 129/378\n",
      "Processing article 130/378\n",
      "Processing article 131/378\n",
      "Processing article 132/378\n",
      "Processing article 133/378\n",
      "Processing article 134/378\n",
      "Processing article 135/378\n",
      "Processing article 136/378\n",
      "Processing article 137/378\n",
      "Processing article 138/378\n",
      "Processing article 139/378\n",
      "Processing article 140/378\n",
      "Processing article 141/378\n",
      "Processing article 142/378\n",
      "Processing article 143/378\n",
      "Processing article 144/378\n",
      "Processing article 145/378\n",
      "Processing article 146/378\n",
      "Processing article 147/378\n",
      "Processing article 148/378\n",
      "Processing article 149/378\n",
      "Processing article 150/378\n",
      "Processing article 151/378\n",
      "Processing article 152/378\n",
      "Processing article 153/378\n",
      "Processing article 154/378\n",
      "Processing article 155/378\n",
      "Processing article 156/378\n",
      "Processing article 157/378\n",
      "Processing article 158/378\n",
      "Processing article 159/378\n",
      "Processing article 160/378\n",
      "Processing article 161/378\n",
      "Processing article 162/378\n",
      "Processing article 163/378\n",
      "Processing article 164/378\n",
      "Processing article 165/378\n",
      "Processing article 166/378\n",
      "Processing article 167/378\n",
      "Processing article 168/378\n",
      "Processing article 169/378\n",
      "Processing article 170/378\n",
      "Processing article 171/378\n",
      "Processing article 172/378\n",
      "Processing article 173/378\n",
      "Processing article 174/378\n",
      "Processing article 175/378\n",
      "Processing article 176/378\n",
      "Processing article 177/378\n",
      "Processing article 178/378\n",
      "Processing article 179/378\n",
      "Processing article 180/378\n",
      "Processing article 181/378\n",
      "Processing article 182/378\n",
      "Processing article 183/378\n",
      "Processing article 184/378\n",
      "Processing article 185/378\n",
      "Processing article 186/378\n",
      "Processing article 187/378\n",
      "Processing article 188/378\n",
      "Processing article 189/378\n",
      "Processing article 190/378\n",
      "Processing article 191/378\n",
      "Processing article 192/378\n",
      "Processing article 193/378\n",
      "Processing article 194/378\n",
      "Processing article 195/378\n",
      "Processing article 196/378\n",
      "Processing article 197/378\n",
      "Processing article 198/378\n",
      "Processing article 199/378\n",
      "Processing article 200/378\n",
      "Processing article 201/378\n",
      "Processing article 202/378\n",
      "Processing article 203/378\n",
      "Processing article 204/378\n",
      "Processing article 205/378\n",
      "Processing article 206/378\n",
      "Processing article 207/378\n",
      "Processing article 208/378\n",
      "Processing article 209/378\n",
      "Processing article 210/378\n",
      "Processing article 211/378\n",
      "Processing article 212/378\n",
      "Processing article 213/378\n",
      "Processing article 214/378\n",
      "Processing article 215/378\n",
      "Processing article 216/378\n",
      "Processing article 217/378\n",
      "Processing article 218/378\n",
      "Processing article 219/378\n",
      "Processing article 220/378\n",
      "Processing article 221/378\n",
      "Processing article 222/378\n",
      "Processing article 223/378\n",
      "Processing article 224/378\n",
      "Processing article 225/378\n",
      "Processing article 226/378\n",
      "Processing article 227/378\n",
      "Processing article 228/378\n",
      "Processing article 229/378\n",
      "Processing article 230/378\n",
      "Processing article 231/378\n",
      "Processing article 232/378\n",
      "Processing article 233/378\n",
      "Processing article 234/378\n",
      "Processing article 235/378\n",
      "Processing article 236/378\n",
      "Processing article 237/378\n",
      "Processing article 238/378\n",
      "Processing article 239/378\n",
      "Processing article 240/378\n",
      "Processing article 241/378\n",
      "Processing article 242/378\n",
      "Processing article 243/378\n",
      "Processing article 244/378\n",
      "Processing article 245/378\n",
      "Processing article 246/378\n",
      "Processing article 247/378\n",
      "Processing article 248/378\n",
      "Processing article 249/378\n",
      "Processing article 250/378\n",
      "Processing article 251/378\n",
      "Processing article 252/378\n",
      "Processing article 253/378\n",
      "Processing article 254/378\n",
      "Processing article 255/378\n",
      "Processing article 256/378\n",
      "Processing article 257/378\n",
      "Processing article 258/378\n",
      "Processing article 259/378\n",
      "Processing article 260/378\n",
      "Processing article 261/378\n",
      "Processing article 262/378\n",
      "Processing article 263/378\n",
      "Processing article 264/378\n",
      "Processing article 265/378\n",
      "Processing article 266/378\n",
      "Processing article 267/378\n",
      "Processing article 268/378\n",
      "Processing article 269/378\n",
      "Processing article 270/378\n",
      "Processing article 271/378\n",
      "Processing article 272/378\n",
      "Processing article 273/378\n",
      "Processing article 274/378\n",
      "Processing article 275/378\n",
      "Processing article 276/378\n",
      "Processing article 277/378\n",
      "Processing article 278/378\n",
      "Processing article 279/378\n",
      "Processing article 280/378\n",
      "Processing article 281/378\n",
      "Processing article 282/378\n",
      "Processing article 283/378\n",
      "Processing article 284/378\n",
      "Processing article 285/378\n",
      "Processing article 286/378\n",
      "Processing article 287/378\n",
      "Processing article 288/378\n",
      "Processing article 289/378\n",
      "Processing article 290/378\n",
      "Processing article 291/378\n",
      "Processing article 292/378\n",
      "Processing article 293/378\n",
      "Processing article 294/378\n",
      "Processing article 295/378\n",
      "Processing article 296/378\n",
      "Processing article 297/378\n",
      "Processing article 298/378\n",
      "Processing article 299/378\n",
      "Processing article 300/378\n",
      "Processing article 301/378\n",
      "Processing article 302/378\n",
      "Processing article 303/378\n",
      "Processing article 304/378\n",
      "Processing article 305/378\n",
      "Processing article 306/378\n",
      "Processing article 307/378\n",
      "Processing article 308/378\n",
      "Processing article 309/378\n",
      "Processing article 310/378\n",
      "Processing article 311/378\n",
      "Processing article 312/378\n",
      "Processing article 313/378\n",
      "Processing article 314/378\n",
      "Processing article 315/378\n",
      "Processing article 316/378\n",
      "Processing article 317/378\n",
      "Processing article 318/378\n",
      "Processing article 319/378\n",
      "Processing article 320/378\n",
      "Processing article 321/378\n",
      "Processing article 322/378\n",
      "Processing article 323/378\n",
      "Processing article 324/378\n",
      "Processing article 325/378\n",
      "Processing article 326/378\n",
      "Processing article 327/378\n",
      "Processing article 328/378\n",
      "Processing article 329/378\n",
      "Processing article 330/378\n",
      "Processing article 331/378\n",
      "Processing article 332/378\n",
      "Processing article 333/378\n",
      "Processing article 334/378\n",
      "Processing article 335/378\n",
      "Processing article 336/378\n",
      "Processing article 337/378\n",
      "Processing article 338/378\n",
      "Processing article 339/378\n",
      "Processing article 340/378\n",
      "Processing article 341/378\n",
      "Processing article 342/378\n",
      "Processing article 343/378\n",
      "Processing article 344/378\n",
      "Processing article 345/378\n",
      "Processing article 346/378\n",
      "Processing article 347/378\n",
      "Processing article 348/378\n",
      "Processing article 349/378\n",
      "Processing article 350/378\n",
      "Processing article 351/378\n",
      "Processing article 352/378\n",
      "Processing article 353/378\n",
      "Processing article 354/378\n",
      "Processing article 355/378\n",
      "Processing article 356/378\n",
      "Processing article 357/378\n",
      "Processing article 358/378\n",
      "Processing article 359/378\n",
      "Processing article 360/378\n",
      "Processing article 361/378\n",
      "Processing article 362/378\n",
      "Processing article 363/378\n",
      "Processing article 364/378\n",
      "Processing article 365/378\n",
      "Processing article 366/378\n",
      "Processing article 367/378\n",
      "Processing article 368/378\n",
      "Processing article 369/378\n",
      "Processing article 370/378\n",
      "Processing article 371/378\n",
      "Processing article 372/378\n",
      "Processing article 373/378\n",
      "Processing article 374/378\n",
      "Processing article 375/378\n",
      "Processing article 376/378\n",
      "Processing article 377/378\n",
      "Processing article 378/378\n",
      "\n",
      "Processing completed!\n",
      "Total articles processed: 378\n",
      "Articles with abstracts: 378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Previous studies have revealed that infants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Most animals can discriminate between pairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Evidence for the anticipation of environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Automatic acquisition of action effect asso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Humans represent perceptual events in a dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Previous research has shown that highly pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Language switching is omnipresent in biling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Korean deaf signers performed a number comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;The process of learning symbolic Arabic dig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;In this study, it is demonstrated that movi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Title  Year                                               Link  \\\n",
       "0    None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "1    None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "2    None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "3    None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "4    None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "..    ...   ...                                                ...   \n",
       "373  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "374  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "375  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "376  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "377  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "\n",
       "                                              Abstract  \n",
       "0    <p>Previous studies have revealed that infants...  \n",
       "1    <p>Most animals can discriminate between pairs...  \n",
       "2    <p>Evidence for the anticipation of environmen...  \n",
       "3    <p>Automatic acquisition of action effect asso...  \n",
       "4    <p>Humans represent perceptual events in a dis...  \n",
       "..                                                 ...  \n",
       "373  <p>Previous research has shown that highly pro...  \n",
       "374  <p>Language switching is omnipresent in biling...  \n",
       "375  <p>Korean deaf signers performed a number comp...  \n",
       "376  <p>The process of learning symbolic Arabic dig...  \n",
       "377  <p>In this study, it is demonstrated that movi...  \n",
       "\n",
       "[378 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[403, 408, 429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "def extract_article_details(session, url, retries=3):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            ## Extract authors\n",
    "            #authors = soup.select('.authors .author-wrapper')\n",
    "            #author_list = [author.get_text(strip=True) for author in authors]\n",
    "            \n",
    "            # Extract abstract\n",
    "            abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "            abstract = abstract_meta['content'] if abstract_meta else \"Abstract not available\"\n",
    "            \n",
    "            # Extract title from meta tag (more reliable)\n",
    "            title_meta = soup.find('meta', attrs={'name': 'citation_title'})\n",
    "            title = title_meta['content'] if title_meta else \"Title not available\"\n",
    "            \n",
    "            return {\n",
    "                'Title': title,\n",
    "                #'Authors': '; '.join(author_list) if author_list else \"Authors not available\",\n",
    "                'Abstract': abstract\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                return {\n",
    "                    'Title': \"Error extracting title\",\n",
    "                    #'Authors': \"Error extracting authors\",\n",
    "                    'Abstract': \"Error extracting abstract\"\n",
    "                }\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "def process_articles(research_df):\n",
    "    session = create_session()\n",
    "    processed_data = []\n",
    "    total_articles = len(research_df)\n",
    "    \n",
    "    for idx, row in research_df.iterrows():\n",
    "        print(f\"Processing article {idx + 1}/{total_articles}\")\n",
    "        \n",
    "        article_data = {\n",
    "            'Title': row['Title'],\n",
    "            'Year': row['Year'],\n",
    "            'Link': row['Link']\n",
    "        }\n",
    "        \n",
    "        if row['Link']:\n",
    "            details = extract_article_details(session, row['Link'])\n",
    "            article_data.update({\n",
    "                #'Authors': details['Authors'],\n",
    "                'Abstract': details['Abstract']\n",
    "            })\n",
    "        else:\n",
    "            article_data.update({\n",
    "                #'Authors': \"Link not available\",\n",
    "                'Abstract': \"Link not available\"\n",
    "            })\n",
    "        \n",
    "        processed_data.append(article_data)\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Process the articles and create final dataset\n",
    "try:\n",
    "    if 'Title' not in research_df.columns or 'Year' not in research_df.columns or 'Link' not in research_df.columns:\n",
    "        raise ValueError(\"Required columns (Title, Year, Link) not found in research_df\")\n",
    "    \n",
    "    final_df = process_articles(research_df)\n",
    "    \n",
    "    print(\"\\nProcessing completed!\")\n",
    "    print(f\"Total articles processed: {len(final_df)}\")\n",
    "    #print(f\"Articles with authors: {final_df['Authors'].notna().sum()}\")\n",
    "    print(f\"Articles with abstracts: {final_df['Abstract'].notna().sum()}\")\n",
    "\n",
    "    # Display DataFrame using IPython display (for Jupyter)\n",
    "    from IPython.display import display\n",
    "    display(final_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf499f89-6246-4173-8666-f8ae888f5db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Link</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Previous studies have revealed that infants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Most animals can discriminate between pairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Evidence for the anticipation of environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Automatic acquisition of action effect asso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://www.frontiersin.org/journals/psycholog...</td>\n",
       "      <td>&lt;p&gt;Humans represent perceptual events in a dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Title  Year                                               Link  \\\n",
       "0  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "1  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "2  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "3  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "4  None  2012  https://www.frontiersin.org/journals/psycholog...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  <p>Previous studies have revealed that infants...  \n",
       "1  <p>Most animals can discriminate between pairs...  \n",
       "2  <p>Evidence for the anticipation of environmen...  \n",
       "3  <p>Automatic acquisition of action effect asso...  \n",
       "4  <p>Humans represent perceptual events in a dis...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = final_df\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c6f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Remove HTML tags (including <p> and </p>)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    \n",
    "    # Remove numbers, asterisks, and daggers\n",
    "    text = re.sub(r'(\\d+|\\*|†)', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean Title and Abstract\n",
    "#result_df['Title'] = result_df['Title'].apply(clean_text)\n",
    "result_df['Abstract'] = result_df['Abstract'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df['Authors'] = result_df['Authors'].apply(\n",
    "#    lambda x: re.sub(r'(\\d+|\\*|†)', '', x)  # Remove digits, asterisks, and dagger symbols\n",
    "#                .replace(\"??\", \"\")          # Remove any question marks\n",
    "#                .replace(\",,\", \",\")         # Consolidate multiple commas\n",
    "#                .replace(\", ,\", \",\")        # Remove spaced commas\n",
    "#                .replace(\"  \", \" \")         # Remove double spaces\n",
    "#                .replace(\", ,\", \",\")                # Remove any trailing commas or spaces\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7545a69-5c3b-4dd1-a78a-e229a5a31c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year                                           Abstract\n",
      "0    2012  Previous studies have revealed that infants ag...\n",
      "1    2012  Most animals can discriminate between pairs of...\n",
      "2    2012  Evidence for the anticipation of environmental...\n",
      "3    2012  Automatic acquisition of action effect associa...\n",
      "4    2012  Humans represent perceptual events in a distri...\n",
      "..    ...                                                ...\n",
      "373  2012  Previous research has shown that highly profic...\n",
      "374  2012  Language switching is omnipresent in bilingual...\n",
      "375  2012  Korean deaf signers performed a number compari...\n",
      "376  2012  The process of learning symbolic Arabic digits...\n",
      "377  2012  In this study, it is demonstrated that moving ...\n",
      "\n",
      "[378 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result_df[['Year', 'Abstract']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1da9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"articles_data_12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5437233",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5437fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
