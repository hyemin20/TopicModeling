{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc20827-376e-4d07-b583-bdd2155ad9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ef4020-8c15-4bbd-abed-ce428862d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PsychologyResearchAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.tokenizer = None\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "        \n",
    "    def preprocess_data(self, text_column='Abstract', year_column='Year'):\n",
    "\n",
    "        # Basic text cleaning\n",
    "        self.df[text_column] = self.df[text_column].astype(str).str.lower()\n",
    "        self.df[text_column] = self.df[text_column].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "        \n",
    "        # Tokenization\n",
    "        self.tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "        self.tokenizer.fit_on_texts(self.df[text_column])\n",
    "        \n",
    "        # Set vocab size explicitly\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        \n",
    "        # Convert text to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(self.df[text_column])\n",
    "        \n",
    "        # Padding sequences\n",
    "        self.max_length = max(len(seq) for seq in sequences)\n",
    "        self.padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        # Encode publication years\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df['year_encoded'] = label_encoder.fit_transform(self.df[year_column])\n",
    "        \n",
    "        return self.padded_sequences, self.df['year_encoded']\n",
    "    \n",
    "    def create_transformer_model(self, embedding_dim=128, num_heads=8, ff_dim=32):\n",
    "        \"\"\"\n",
    "        Create a Transformer-based model for time series prediction\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.Input(shape=(self.max_length,))\n",
    "        \n",
    "        # Embedding Layer with explicit vocab_size and input_length\n",
    "        x = tf.keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size, \n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        # Flatten the embedding layer\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X, y, test_size=0.2, epochs=50):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = self.create_transformer_model()\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea8e176-8064-48e9-a997-47b8d863f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - loss: 15.3282 - val_loss: 7.4309\n",
      "Epoch 2/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - loss: 4.0388 - val_loss: 6.0377\n",
      "Epoch 3/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - loss: 0.9565 - val_loss: 5.8227\n",
      "Epoch 4/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.3907 - val_loss: 6.0750\n",
      "Epoch 5/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - loss: 0.3182 - val_loss: 5.9556\n",
      "Epoch 6/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - loss: 0.4395 - val_loss: 5.9854\n",
      "Epoch 7/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - loss: 0.4729 - val_loss: 5.8825\n",
      "Epoch 8/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.3857 - val_loss: 5.8020\n",
      "Epoch 9/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.2934 - val_loss: 5.8402\n",
      "Epoch 10/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.2637 - val_loss: 5.7816\n",
      "Epoch 11/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.2764 - val_loss: 5.8511\n",
      "Epoch 12/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.3102 - val_loss: 5.8441\n",
      "Epoch 13/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.2665 - val_loss: 5.7770\n",
      "Epoch 14/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - loss: 0.2352 - val_loss: 5.7622\n",
      "Epoch 15/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.2170 - val_loss: 5.7986\n",
      "Epoch 16/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.2573 - val_loss: 5.7625\n",
      "Epoch 17/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - loss: 0.1897 - val_loss: 5.7428\n",
      "Epoch 18/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - loss: 0.1913 - val_loss: 5.7383\n",
      "Epoch 19/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - loss: 0.1825 - val_loss: 5.7576\n",
      "Epoch 20/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.1673 - val_loss: 5.7234\n",
      "Epoch 21/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.1871 - val_loss: 5.7643\n",
      "Epoch 22/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1662 - val_loss: 5.7158\n",
      "Epoch 23/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.1491 - val_loss: 5.6868\n",
      "Epoch 24/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1373 - val_loss: 5.7278\n",
      "Epoch 25/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1434 - val_loss: 5.6841\n",
      "Epoch 26/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.1362 - val_loss: 5.6703\n",
      "Epoch 27/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - loss: 0.1289 - val_loss: 5.6688\n",
      "Epoch 28/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.1263 - val_loss: 5.7022\n",
      "Epoch 29/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1298 - val_loss: 5.6757\n",
      "Epoch 30/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.1160 - val_loss: 5.6734\n",
      "Epoch 31/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.1044 - val_loss: 5.7080\n",
      "Epoch 32/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.1105 - val_loss: 5.6765\n",
      "Epoch 33/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1178 - val_loss: 5.6627\n",
      "Epoch 34/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.0956 - val_loss: 5.6680\n",
      "Epoch 35/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.1131 - val_loss: 5.6584\n",
      "Epoch 36/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - loss: 0.0998 - val_loss: 5.6429\n",
      "Epoch 37/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - loss: 0.0966 - val_loss: 5.7054\n",
      "Epoch 38/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 41ms/step - loss: 0.0821 - val_loss: 5.6521\n",
      "Epoch 39/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 36ms/step - loss: 0.0859 - val_loss: 5.6737\n",
      "Epoch 40/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.0860 - val_loss: 5.6619\n",
      "Epoch 41/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 42ms/step - loss: 0.0905 - val_loss: 5.6576\n",
      "Epoch 42/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.0909 - val_loss: 5.6925\n",
      "Epoch 43/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.0921 - val_loss: 5.6357\n",
      "Epoch 44/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.0749 - val_loss: 5.6269\n",
      "Epoch 45/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - loss: 0.0714 - val_loss: 5.6824\n",
      "Epoch 46/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - loss: 0.0737 - val_loss: 5.6558\n",
      "Epoch 47/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - loss: 0.0863 - val_loss: 5.7190\n",
      "Epoch 48/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - loss: 0.0836 - val_loss: 5.6347\n",
      "Epoch 49/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - loss: 0.0659 - val_loss: 5.6330\n",
      "Epoch 50/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 39ms/step - loss: 0.0610 - val_loss: 5.6365\n"
     ]
    }
   ],
   "source": [
    "analyzer = PsychologyResearchAnalyzer('articles_tokenize.csv')\n",
    "X, y = analyzer.preprocess_data()\n",
    "model, training_history = analyzer.train_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f2813-c15e-4c72-a2b9-a2fc6901e16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e0f193-49ff-408a-8939-0b1333ef9976",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mPsychologyResearchAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticles_tokenize.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Extract named entities\u001b[39;00m\n\u001b[0;32m    115\u001b[0m named_entities \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mextract_named_entities()\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mPsychologyResearchAnalyzer.__init__\u001b[1;34m(self, data_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load XLM-RoBERTa NER model and tokenizer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large-finetuned-conll03-english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large-finetuned-conll03-english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_model, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_tokenizer)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Additional attributes for ML processing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1669\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class PsychologyResearchAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Load XLM-RoBERTa NER model and tokenizer\n",
    "        self.ner_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_pipeline = pipeline(\"ner\", model=self.ner_model, tokenizer=self.ner_tokenizer)\n",
    "        \n",
    "        # Additional attributes for ML processing\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "    \n",
    "    def extract_named_entities(self, text_column='Abstract_tokens'):\n",
    "\n",
    "        # Apply NER to each abstract\n",
    "        self.df['named_entities'] = self.df[text_column].apply(\n",
    "            lambda x: self.ner_pipeline(str(x))\n",
    "        )\n",
    "        \n",
    "        # Create separate columns for different entity types\n",
    "        self.df['persons'] = self.df['named_entities'].apply(\n",
    "            lambda entities: [e['word'] for e in entities if e['entity'] == 'I-PER']\n",
    "        )\n",
    "        self.df['locations'] = self.df['named_entities'].apply(\n",
    "            lambda entities: [e['word'] for e in entities if e['entity'] == 'I-LOC']\n",
    "        )\n",
    "        \n",
    "        return self.df[['persons', 'locations']]\n",
    "    \n",
    "    def preprocess_data(self, text_column='Abstract', year_column='Year'):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with XLM-RoBERTa tokenization\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        self.df[text_column] = self.df[text_column].astype(str).str.lower()\n",
    "        \n",
    "        # Tokenize using XLM-RoBERTa tokenizer\n",
    "        encoded_inputs = self.ner_tokenizer(\n",
    "            self.df[text_column].tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Set max length and vocab size\n",
    "        self.max_length = encoded_inputs['input_ids'].shape[1]\n",
    "        self.vocab_size = self.ner_tokenizer.vocab_size\n",
    "        \n",
    "        # Encode publication years\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df['year_encoded'] = label_encoder.fit_transform(self.df[year_column])\n",
    "        \n",
    "        return encoded_inputs, self.df['year_encoded']\n",
    "    \n",
    "    def create_transformer_model(self, embedding_dim=128, num_heads=8, ff_dim=32):\n",
    "        \"\"\"\n",
    "        Create a Transformer-based model with XLM-RoBERTa inspired architecture\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.Input(shape=(self.max_length,))\n",
    "        \n",
    "        # Use pre-trained embedding layer concept from XLM-RoBERTa\n",
    "        x = tf.keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size, \n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        # Multi-head attention inspired layers\n",
    "        x = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embedding_dim\n",
    "        )(x, x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X, y, test_size=0.2, epochs=50):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X['input_ids'], y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = self.create_transformer_model()\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "# Example usage\n",
    "analyzer = PsychologyResearchAnalyzer('articles_tokenize.csv')\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = analyzer.extract_named_entities()\n",
    "\n",
    "# Preprocess data\n",
    "X, y = analyzer.preprocess_data()\n",
    "\n",
    "# Train model\n",
    "model, training_history = analyzer.train_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0b22ad-d8e1-43af-8e56-dabf4975b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hyemi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 3.7/9.7 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.7 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 20.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.17.0 fsspec-2024.12.0 huggingface-hub-0.27.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ebecab-6a7d-4dd8-a9f3-5663b89678ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cv_scores\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mPsychologyResearchAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticles_tokenize.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m    134\u001b[0m X, y \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mpreprocess_data()\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mPsychologyResearchAnalyzer.__init__\u001b[1;34m(self, data_path)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load XLM-RoBERTa NER model and tokenizer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large-finetuned-conll03-english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-large-finetuned-conll03-english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_model, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mner_tokenizer)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Additional attributes for ML processing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1669\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class PsychologyResearchAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Load XLM-RoBERTa NER model and tokenizer\n",
    "        self.ner_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_pipeline = pipeline(\"ner\", model=self.ner_model, tokenizer=self.ner_tokenizer)\n",
    "        \n",
    "        # Additional attributes for ML processing\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "    \n",
    "    def preprocess_data(self, text_column='Abstract_tokens', year_column='Year'):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with XLM-RoBERTa tokenization\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        self.df[text_column] = self.df[text_column].astype(str).str.lower()\n",
    "        \n",
    "        # Tokenize using XLM-RoBERTa tokenizer\n",
    "        encoded_inputs = self.ner_tokenizer(\n",
    "            self.df[text_column].tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Set max length and vocab size\n",
    "        self.max_length = encoded_inputs['input_ids'].shape[1]\n",
    "        self.vocab_size = self.ner_tokenizer.vocab_size\n",
    "        \n",
    "        # Encode publication years\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df['year_encoded'] = label_encoder.fit_transform(self.df[year_column])\n",
    "        \n",
    "        return encoded_inputs, self.df['year_encoded']\n",
    "    \n",
    "    def create_transformer_model(self, embedding_dim=128, num_heads=8, ff_dim=32):\n",
    "        \"\"\"\n",
    "        Create a Transformer-based model with XLM-RoBERTa inspired architecture\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.Input(shape=(self.max_length,))\n",
    "        \n",
    "        x = tf.keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size, \n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        x = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embedding_dim\n",
    "        )(x, x)\n",
    "        \n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def cross_validate(self, X, y, n_splits=5, epochs=50):\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation with comprehensive metrics\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        input_ids = X['input_ids']\n",
    "        \n",
    "        # Initialize cross-validation\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Metrics storage\n",
    "        cv_scores = {\n",
    "            'mse': [],\n",
    "            'r2': []\n",
    "        }\n",
    "        \n",
    "        # Fold-wise model training and evaluation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kfold.split(input_ids), 1):\n",
    "            # Split data\n",
    "            X_train, X_val = input_ids[train_indices], input_ids[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_transformer_model()\n",
    "            model.fit(\n",
    "                X_train, y_train, \n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(X_val).flatten()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            \n",
    "            # Store metrics\n",
    "            cv_scores['mse'].append(mse)\n",
    "            cv_scores['r2'].append(r2)\n",
    "            \n",
    "            print(f\"Fold {fold}: MSE = {mse:.4f}, R² = {r2:.4f}\")\n",
    "        \n",
    "        # Compute average cross-validation scores\n",
    "        avg_mse = np.mean(cv_scores['mse'])\n",
    "        avg_r2 = np.mean(cv_scores['r2'])\n",
    "        \n",
    "        print(\"\\nCross-Validation Results:\")\n",
    "        print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "        print(f\"Average R²: {avg_r2:.4f}\")\n",
    "        \n",
    "        return cv_scores\n",
    "\n",
    "# Example usage\n",
    "analyzer = PsychologyResearchAnalyzer('articles_tokenize.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X, y = analyzer.preprocess_data()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = analyzer.cross_validate(X, y, n_splits=5)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c455c-7dd8-452d-ae36-8cbdcddbc696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
