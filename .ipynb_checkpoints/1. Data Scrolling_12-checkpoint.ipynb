{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eac5082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hyemi\\\\Python\\\\TopicModeling'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007a03a",
   "metadata": {},
   "source": [
    "# Frontier in Psychology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712807f3-b4f8-481c-a49d-0279316e9353",
   "metadata": {},
   "source": [
    "# 1.background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdefa19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6164f5",
   "metadata": {},
   "source": [
    "##### Bring URL and HTML\n",
    "url = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3bcdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Convert BeautifulSoup object to string\n",
    "html_content = str(soup)\n",
    "\n",
    "##### Search for title pattern\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html_content, re.IGNORECASE)\n",
    "title = match_results.group() if match_results else \"No title found\"\n",
    "title = re.sub(\"<.*?>\", \"\", title)  # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e360e6",
   "metadata": {},
   "source": [
    "##### https://www.frontiersin.org/journals/psychology/articles?publication-date=25%2F10%2F2024-25%2F10%2F2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f6af844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles loaded: 32, Retries: 0\n",
      "Articles loaded: 48, Retries: 0\n",
      "Articles loaded: 64, Retries: 0\n",
      "Articles loaded: 80, Retries: 0\n",
      "Articles loaded: 96, Retries: 0\n",
      "Articles loaded: 112, Retries: 0\n",
      "Articles loaded: 128, Retries: 0\n",
      "Articles loaded: 144, Retries: 0\n",
      "Articles loaded: 160, Retries: 0\n",
      "Articles loaded: 176, Retries: 0\n",
      "Articles loaded: 192, Retries: 0\n",
      "Articles loaded: 208, Retries: 0\n",
      "Articles loaded: 224, Retries: 0\n",
      "Articles loaded: 240, Retries: 0\n",
      "Articles loaded: 256, Retries: 0\n",
      "Articles loaded: 272, Retries: 0\n",
      "Articles loaded: 288, Retries: 0\n",
      "Articles loaded: 304, Retries: 0\n",
      "Articles loaded: 320, Retries: 0\n",
      "Articles loaded: 336, Retries: 0\n",
      "Articles loaded: 352, Retries: 0\n",
      "Articles loaded: 368, Retries: 0\n",
      "Articles loaded: 384, Retries: 0\n",
      "Articles loaded: 400, Retries: 0\n",
      "Articles loaded: 416, Retries: 0\n",
      "Articles loaded: 432, Retries: 0\n",
      "Articles loaded: 448, Retries: 0\n",
      "Articles loaded: 464, Retries: 0\n",
      "Articles loaded: 480, Retries: 0\n",
      "Articles loaded: 496, Retries: 0\n",
      "Articles loaded: 512, Retries: 0\n",
      "Articles loaded: 528, Retries: 0\n",
      "Articles loaded: 544, Retries: 0\n",
      "Articles loaded: 560, Retries: 0\n",
      "Articles loaded: 576, Retries: 0\n",
      "Articles loaded: 592, Retries: 0\n",
      "Articles loaded: 608, Retries: 0\n",
      "Articles loaded: 624, Retries: 0\n",
      "Articles loaded: 640, Retries: 0\n",
      "Articles loaded: 656, Retries: 0\n",
      "Articles loaded: 672, Retries: 0\n",
      "Articles loaded: 688, Retries: 0\n",
      "Articles loaded: 704, Retries: 0\n",
      "Articles loaded: 720, Retries: 0\n",
      "Articles loaded: 736, Retries: 0\n",
      "Articles loaded: 752, Retries: 0\n",
      "Articles loaded: 768, Retries: 0\n",
      "Articles loaded: 784, Retries: 0\n",
      "Articles loaded: 800, Retries: 0\n",
      "Articles loaded: 816, Retries: 0\n",
      "Articles loaded: 832, Retries: 0\n",
      "Articles loaded: 848, Retries: 0\n",
      "Articles loaded: 864, Retries: 0\n",
      "Articles loaded: 880, Retries: 0\n",
      "Articles loaded: 896, Retries: 0\n",
      "Articles loaded: 912, Retries: 0\n",
      "Articles loaded: 928, Retries: 0\n",
      "Articles loaded: 944, Retries: 0\n",
      "Articles loaded: 960, Retries: 0\n",
      "Articles loaded: 976, Retries: 0\n",
      "Articles loaded: 992, Retries: 0\n",
      "Articles loaded: 1008, Retries: 0\n",
      "Articles loaded: 1024, Retries: 0\n",
      "Articles loaded: 1040, Retries: 0\n",
      "Articles loaded: 1056, Retries: 0\n",
      "Articles loaded: 1072, Retries: 0\n",
      "Articles loaded: 1088, Retries: 0\n",
      "Articles loaded: 1104, Retries: 0\n",
      "Articles loaded: 1120, Retries: 0\n",
      "Articles loaded: 1136, Retries: 0\n",
      "Articles loaded: 1152, Retries: 0\n",
      "Articles loaded: 1168, Retries: 0\n",
      "Articles loaded: 1184, Retries: 0\n",
      "Articles loaded: 1200, Retries: 0\n",
      "Articles loaded: 1216, Retries: 0\n",
      "Articles loaded: 1232, Retries: 0\n",
      "Articles loaded: 1248, Retries: 0\n",
      "Articles loaded: 1264, Retries: 0\n",
      "Articles loaded: 1280, Retries: 0\n",
      "Articles loaded: 1296, Retries: 0\n",
      "Articles loaded: 1312, Retries: 0\n",
      "Articles loaded: 1328, Retries: 0\n",
      "Articles loaded: 1344, Retries: 0\n",
      "Articles loaded: 1360, Retries: 0\n",
      "Articles loaded: 1376, Retries: 0\n",
      "Articles loaded: 1392, Retries: 0\n",
      "Articles loaded: 1408, Retries: 0\n",
      "Articles loaded: 1424, Retries: 0\n",
      "Articles loaded: 1440, Retries: 0\n",
      "Articles loaded: 1456, Retries: 0\n",
      "Articles loaded: 1472, Retries: 0\n",
      "Articles loaded: 1488, Retries: 0\n",
      "Articles loaded: 1504, Retries: 0\n",
      "Articles loaded: 1520, Retries: 0\n",
      "Articles loaded: 1536, Retries: 0\n",
      "Articles loaded: 1552, Retries: 0\n",
      "Articles loaded: 1568, Retries: 0\n",
      "Articles loaded: 1584, Retries: 0\n",
      "Articles loaded: 1600, Retries: 0\n",
      "Articles loaded: 1616, Retries: 0\n",
      "Articles loaded: 1632, Retries: 0\n",
      "Articles loaded: 1648, Retries: 0\n",
      "Articles loaded: 1664, Retries: 0\n",
      "Articles loaded: 1680, Retries: 0\n",
      "Articles loaded: 1696, Retries: 0\n",
      "Articles loaded: 1712, Retries: 0\n",
      "Articles loaded: 1728, Retries: 0\n",
      "Articles loaded: 1744, Retries: 0\n",
      "Articles loaded: 1760, Retries: 0\n",
      "Articles loaded: 1776, Retries: 0\n",
      "Articles loaded: 1792, Retries: 0\n",
      "Articles loaded: 1808, Retries: 0\n",
      "Articles loaded: 1824, Retries: 0\n",
      "Articles loaded: 1840, Retries: 0\n",
      "Articles loaded: 1856, Retries: 0\n",
      "Articles loaded: 1872, Retries: 0\n",
      "Articles loaded: 1888, Retries: 0\n",
      "Articles loaded: 1904, Retries: 0\n",
      "Articles loaded: 1920, Retries: 0\n",
      "Articles loaded: 1936, Retries: 0\n",
      "Articles loaded: 1952, Retries: 0\n",
      "Articles loaded: 1968, Retries: 0\n",
      "Articles loaded: 1984, Retries: 0\n",
      "Articles loaded: 2000, Retries: 0\n",
      "Articles loaded: 2016, Retries: 0\n",
      "Articles loaded: 2032, Retries: 0\n",
      "Articles loaded: 2048, Retries: 0\n",
      "Articles loaded: 2064, Retries: 0\n",
      "Articles loaded: 2080, Retries: 0\n",
      "Articles loaded: 2096, Retries: 0\n",
      "Articles loaded: 2112, Retries: 0\n",
      "Articles loaded: 2128, Retries: 0\n",
      "Articles loaded: 2144, Retries: 0\n",
      "Articles loaded: 2160, Retries: 0\n",
      "Articles loaded: 2176, Retries: 0\n",
      "Articles loaded: 2192, Retries: 0\n",
      "Articles loaded: 2208, Retries: 0\n",
      "Articles loaded: 2224, Retries: 0\n",
      "Articles loaded: 2240, Retries: 0\n",
      "Articles loaded: 2256, Retries: 0\n",
      "Articles loaded: 2272, Retries: 0\n",
      "Articles loaded: 2288, Retries: 0\n",
      "Articles loaded: 2304, Retries: 0\n",
      "Articles loaded: 2320, Retries: 0\n",
      "Articles loaded: 2336, Retries: 0\n",
      "Articles loaded: 2352, Retries: 0\n",
      "Articles loaded: 2368, Retries: 0\n",
      "Articles loaded: 2384, Retries: 0\n",
      "Articles loaded: 2400, Retries: 0\n",
      "Articles loaded: 2416, Retries: 0\n",
      "Articles loaded: 2432, Retries: 0\n",
      "Articles loaded: 2448, Retries: 0\n",
      "Articles loaded: 2464, Retries: 0\n",
      "Articles loaded: 2480, Retries: 0\n",
      "Articles loaded: 2496, Retries: 0\n",
      "Articles loaded: 2512, Retries: 0\n",
      "Articles loaded: 2528, Retries: 0\n",
      "Articles loaded: 2544, Retries: 0\n",
      "Articles loaded: 2560, Retries: 0\n",
      "Articles loaded: 2576, Retries: 0\n",
      "Articles loaded: 2592, Retries: 0\n",
      "Articles loaded: 2608, Retries: 0\n",
      "Articles loaded: 2624, Retries: 0\n",
      "Articles loaded: 2640, Retries: 0\n",
      "Articles loaded: 2656, Retries: 0\n",
      "Articles loaded: 2672, Retries: 0\n",
      "Articles loaded: 2688, Retries: 0\n",
      "Articles loaded: 2704, Retries: 0\n",
      "Articles loaded: 2720, Retries: 0\n",
      "Articles loaded: 2726, Retries: 0\n",
      "Articles loaded: 2726, Retries: 1\n",
      "Articles loaded: 2726, Retries: 2\n",
      "Articles loaded: 2726, Retries: 3\n",
      "Articles loaded: 2726, Retries: 4\n",
      "Articles loaded: 2726, Retries: 5\n",
      "Articles loaded: 2726, Retries: 6\n",
      "Articles loaded: 2726, Retries: 7\n",
      "Articles loaded: 2726, Retries: 8\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Open the page\n",
    "base_url = \"https://www.frontiersin.org/journals/psychology/articles\"\n",
    "url = base_url + \"?publication-date=01%2F01%2F2018-31%2F12%2F2018\"\n",
    "driver.get(url)\n",
    "\n",
    "# Set page zoom to 25%\n",
    "driver.execute_script(\"document.body.style.zoom='25%'\")\n",
    "\n",
    "# Accept cookies if the banner appears\n",
    "try:\n",
    "    accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "    )\n",
    "    accept_cookies_button.click()\n",
    "except:\n",
    "    print(\"No cookie banner detected.\")\n",
    "\n",
    "# Wait for the first article to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"CardArticle\"))\n",
    ")\n",
    "\n",
    "# Scroll and hover to trigger loading of new articles\n",
    "scroll_increment = 200\n",
    "scroll_pause_time = 1.5\n",
    "max_retries = 8\n",
    "retries = 0\n",
    "prev_article_count = 0\n",
    "\n",
    "# Scroll until all articles are loaded\n",
    "actions = ActionChains(driver)\n",
    "while retries < max_retries:\n",
    "    # Scroll by a small increment\n",
    "    driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "    time.sleep(scroll_pause_time)\n",
    "    \n",
    "    # Hover over the footer to trigger lazy loading\n",
    "    footer = driver.find_element(By.TAG_NAME, \"footer\")\n",
    "    actions.move_to_element(footer).perform()\n",
    "    \n",
    "    # Check current number of articles loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all(\"article\", class_=\"CardArticle\")\n",
    "    current_article_count = len(articles)\n",
    "    \n",
    "    # Update retry count based on loaded articles\n",
    "    if current_article_count > prev_article_count:\n",
    "        prev_article_count = current_article_count\n",
    "        retries = 0  # Reset retries if new articles loaded\n",
    "    else:\n",
    "        retries += 1  # Increment retry count if no new articles load\n",
    "    print(f\"Articles loaded: {current_article_count}, Retries: {retries}\")\n",
    "\n",
    "# Ensure elements are fully loaded\n",
    "time.sleep(3)\n",
    "\n",
    "# Extract article information with error handling\n",
    "data = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        # Find elements with None handling\n",
    "        title_elem = article.find(\"h1\", class_=\"CardArticle__title\")\n",
    "        date_elem = article.find(\"p\", class_=\"CardArticle__date\")\n",
    "        type_elem = article.find(\"p\", class_=\"CardArticle__type\")\n",
    "        link_elem = article.find(\"a\", class_=\"CardArticle__wrapper\")\n",
    "        \n",
    "        # Extract data with None checks\n",
    "        title = title_elem.get_text(strip=True) if title_elem else None\n",
    "        publish_date = date_elem.get_text(strip=True) if date_elem else None\n",
    "        article_type = type_elem.get_text(strip=True) if type_elem else None\n",
    "        link = link_elem[\"href\"] if link_elem else None\n",
    "        \n",
    "        data.append({\n",
    "            \"Title\": title,\n",
    "            \"Type\": article_type,\n",
    "            \"Published Date\": publish_date,\n",
    "            \"Link\": link,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "159e75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and process the data\n",
    "def clean_publication_dates(df):\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Clean date strings and convert to datetime\n",
    "    df['Published Date'] = df['Published Date'].str.replace(\n",
    "        r'(Published on |Accepted on )', '', \n",
    "        regex=True\n",
    "    )\n",
    "    \n",
    "    # Convert to datetime with proper error handling\n",
    "    df['Published Date'] = pd.to_datetime(\n",
    "        df['Published Date'],\n",
    "        dayfirst=True,  # Assuming date format is DD/MM/YYYY\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Extract year and handle missing values\n",
    "    df['Year'] = df['Published Date'].dt.strftime('%Y')\n",
    "    df['Year'] = df['Year'].fillna('2014')  # Fill missing years with 2014\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_research_articles(df):\n",
    "    # Filter for Original Research articles\n",
    "    research_df = df[df['Type'] == 'Original Research'].copy()\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    research_df = research_df.reset_index(drop=True)\n",
    "    \n",
    "    return research_df\n",
    "\n",
    "# Process the dataframe\n",
    "processed_df = clean_publication_dates(df)\n",
    "\n",
    "# Filter for Original Research articles\n",
    "research_df = filter_research_articles(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7932307c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dfeac4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da64b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0bda5c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1435688/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#paragraphs = soup.find_all('p')\n",
    "#paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2390d8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1994fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce65f4a-4f71-466b-8c12-4ea6a19a7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 1/1982\n",
      "Processing article 2/1982\n",
      "Processing article 3/1982\n",
      "Processing article 4/1982\n",
      "Processing article 5/1982\n",
      "Processing article 6/1982\n",
      "Processing article 7/1982\n",
      "Processing article 8/1982\n",
      "Processing article 9/1982\n",
      "Processing article 10/1982\n",
      "Processing article 11/1982\n",
      "Processing article 12/1982\n",
      "Processing article 13/1982\n",
      "Processing article 14/1982\n",
      "Processing article 15/1982\n",
      "Processing article 16/1982\n",
      "Processing article 17/1982\n",
      "Processing article 18/1982\n",
      "Processing article 19/1982\n",
      "Processing article 20/1982\n",
      "Processing article 21/1982\n",
      "Processing article 22/1982\n",
      "Processing article 23/1982\n"
     ]
    }
   ],
   "source": [
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[403, 408, 429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "def extract_article_details(session, url, retries=3):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            ## Extract authors\n",
    "            #authors = soup.select('.authors .author-wrapper')\n",
    "            #author_list = [author.get_text(strip=True) for author in authors]\n",
    "            \n",
    "            # Extract abstract\n",
    "            abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "            abstract = abstract_meta['content'] if abstract_meta else \"Abstract not available\"\n",
    "            \n",
    "            # Extract title from meta tag (more reliable)\n",
    "            title_meta = soup.find('meta', attrs={'name': 'citation_title'})\n",
    "            title = title_meta['content'] if title_meta else \"Title not available\"\n",
    "            \n",
    "            return {\n",
    "                'Title': title,\n",
    "                #'Authors': '; '.join(author_list) if author_list else \"Authors not available\",\n",
    "                'Abstract': abstract\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                return {\n",
    "                    'Title': \"Error extracting title\",\n",
    "                    #'Authors': \"Error extracting authors\",\n",
    "                    'Abstract': \"Error extracting abstract\"\n",
    "                }\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "def process_articles(research_df):\n",
    "    session = create_session()\n",
    "    processed_data = []\n",
    "    total_articles = len(research_df)\n",
    "    \n",
    "    for idx, row in research_df.iterrows():\n",
    "        print(f\"Processing article {idx + 1}/{total_articles}\")\n",
    "        \n",
    "        article_data = {\n",
    "            'Title': row['Title'],\n",
    "            'Year': row['Year'],\n",
    "            'Link': row['Link']\n",
    "        }\n",
    "        \n",
    "        if row['Link']:\n",
    "            details = extract_article_details(session, row['Link'])\n",
    "            article_data.update({\n",
    "                #'Authors': details['Authors'],\n",
    "                'Abstract': details['Abstract']\n",
    "            })\n",
    "        else:\n",
    "            article_data.update({\n",
    "                #'Authors': \"Link not available\",\n",
    "                'Abstract': \"Link not available\"\n",
    "            })\n",
    "        \n",
    "        processed_data.append(article_data)\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Process the articles and create final dataset\n",
    "try:\n",
    "    if 'Title' not in research_df.columns or 'Year' not in research_df.columns or 'Link' not in research_df.columns:\n",
    "        raise ValueError(\"Required columns (Title, Year, Link) not found in research_df\")\n",
    "    \n",
    "    final_df = process_articles(research_df)\n",
    "    \n",
    "    print(\"\\nProcessing completed!\")\n",
    "    print(f\"Total articles processed: {len(final_df)}\")\n",
    "    #print(f\"Articles with authors: {final_df['Authors'].notna().sum()}\")\n",
    "    print(f\"Articles with abstracts: {final_df['Abstract'].notna().sum()}\")\n",
    "\n",
    "    # Display DataFrame using IPython display (for Jupyter)\n",
    "    from IPython.display import display\n",
    "    display(final_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf499f89-6246-4173-8666-f8ae888f5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = final_df\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Remove HTML tags (including <p> and </p>)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    \n",
    "    # Remove numbers, asterisks, and daggers\n",
    "    text = re.sub(r'(\\d+|\\*|†)', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean Title and Abstract\n",
    "#result_df['Title'] = result_df['Title'].apply(clean_text)\n",
    "result_df['Abstract'] = result_df['Abstract'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df['Authors'] = result_df['Authors'].apply(\n",
    "#    lambda x: re.sub(r'(\\d+|\\*|†)', '', x)  # Remove digits, asterisks, and dagger symbols\n",
    "#                .replace(\"??\", \"\")          # Remove any question marks\n",
    "#                .replace(\",,\", \",\")         # Consolidate multiple commas\n",
    "#                .replace(\", ,\", \",\")        # Remove spaced commas\n",
    "#                .replace(\"  \", \" \")         # Remove double spaces\n",
    "#                .replace(\", ,\", \",\")                # Remove any trailing commas or spaces\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7545a69-5c3b-4dd1-a78a-e229a5a31c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df[['Year', 'Abstract']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"articles_data_18.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5437233",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5437fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
