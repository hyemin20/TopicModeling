{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc20827-376e-4d07-b583-bdd2155ad9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef4020-8c15-4bbd-abed-ce428862d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PsychologyResearchAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.tokenizer = None\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "        \n",
    "    def preprocess_data(self, text_column='Abstract_tokens', year_column='Year'):\n",
    "\n",
    "        # Basic text cleaning\n",
    "        self.df[text_column] = self.df[text_column].astype(str).str.lower()\n",
    "        self.df[text_column] = self.df[text_column].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "        \n",
    "        # Set vocab size explicitly\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        \n",
    "        # Convert text to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(self.df[text_column])\n",
    "        \n",
    "        # Padding sequences\n",
    "        self.max_length = max(len(seq) for seq in sequences)\n",
    "        self.padded_sequences = pad_sequences(sequences, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        # Encode publication years\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df['year_encoded'] = label_encoder.fit_transform(self.df[year_column])\n",
    "        \n",
    "        return self.padded_sequences, self.df['year_encoded']\n",
    "    \n",
    "    def create_transformer_model(self, embedding_dim=128, num_heads=8, ff_dim=32):\n",
    "        \"\"\"\n",
    "        Create a Transformer-based model for time series prediction\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.Input(shape=(self.max_length,))\n",
    "        \n",
    "        # Embedding Layer with explicit vocab_size and input_length\n",
    "        x = tf.keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size, \n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        # Flatten the embedding layer\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X, y, test_size=0.2, epochs=50):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = self.create_transformer_model()\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8e176-8064-48e9-a997-47b8d863f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - loss: 9.8282 - val_loss: 6.2306\n",
      "Epoch 2/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 3.3426 - val_loss: 6.2149\n",
      "Epoch 3/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.9678 - val_loss: 5.8540\n",
      "Epoch 4/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - loss: 0.3664 - val_loss: 5.8954\n",
      "Epoch 5/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - loss: 0.2289 - val_loss: 5.8431\n",
      "Epoch 6/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.3624 - val_loss: 5.8944\n",
      "Epoch 7/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 48ms/step - loss: 0.5366 - val_loss: 5.8693\n",
      "Epoch 8/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 46ms/step - loss: 0.4863 - val_loss: 5.8285\n",
      "Epoch 9/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 48ms/step - loss: 0.3524 - val_loss: 5.7222\n",
      "Epoch 10/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 49ms/step - loss: 0.2376 - val_loss: 5.6688\n",
      "Epoch 11/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 47ms/step - loss: 0.2421 - val_loss: 5.6728\n",
      "Epoch 12/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.3131 - val_loss: 5.7107\n",
      "Epoch 13/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.3002 - val_loss: 5.6243\n",
      "Epoch 14/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - loss: 0.2424 - val_loss: 5.6701\n",
      "Epoch 15/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.2030 - val_loss: 5.7078\n",
      "Epoch 16/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - loss: 0.1889 - val_loss: 5.6233\n",
      "Epoch 17/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.2086 - val_loss: 5.6638\n",
      "Epoch 18/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.2095 - val_loss: 5.6323\n",
      "Epoch 19/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.2062 - val_loss: 5.6106\n",
      "Epoch 20/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.1638 - val_loss: 5.6248\n",
      "Epoch 21/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - loss: 0.1642 - val_loss: 5.6041\n",
      "Epoch 22/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1445 - val_loss: 5.6085\n",
      "Epoch 23/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - loss: 0.1503 - val_loss: 5.5885\n",
      "Epoch 24/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 49ms/step - loss: 0.1515 - val_loss: 5.5682\n",
      "Epoch 25/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 42ms/step - loss: 0.1234 - val_loss: 5.6262\n",
      "Epoch 26/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - loss: 0.1403 - val_loss: 5.5485\n",
      "Epoch 27/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 44ms/step - loss: 0.1370 - val_loss: 5.5688\n",
      "Epoch 28/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 42ms/step - loss: 0.1143 - val_loss: 5.5722\n",
      "Epoch 29/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - loss: 0.1128 - val_loss: 5.5626\n",
      "Epoch 30/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.1066 - val_loss: 5.5541\n",
      "Epoch 31/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - loss: 0.0961 - val_loss: 5.5351\n",
      "Epoch 32/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.1010 - val_loss: 5.5893\n",
      "Epoch 33/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - loss: 0.1115 - val_loss: 5.5520\n",
      "Epoch 34/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - loss: 0.0922 - val_loss: 5.5456\n",
      "Epoch 35/50\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - loss: 0.0913 - val_loss: 5.5552\n",
      "Epoch 36/50\n",
      "\u001b[1m670/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - loss: 0.0851 "
     ]
    }
   ],
   "source": [
    "analyzer = PsychologyResearchAnalyzer('articles_tokenize.csv')\n",
    "X, y = analyzer.preprocess_data()\n",
    "model, training_history = analyzer.train_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f2813-c15e-4c72-a2b9-a2fc6901e16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0f193-49ff-408a-8939-0b1333ef9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class PsychologyResearchAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Load XLM-RoBERTa NER model and tokenizer\n",
    "        self.ner_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_pipeline = pipeline(\"ner\", model=self.ner_model, tokenizer=self.ner_tokenizer)\n",
    "        \n",
    "        # Additional attributes for ML processing\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "    \n",
    "    def extract_named_entities(self, text_column='Abstract_tokens'):\n",
    "\n",
    "        # Apply NER to each abstract\n",
    "        self.df['named_entities'] = self.df[text_column].apply(\n",
    "            lambda x: self.ner_pipeline(str(x))\n",
    "        )\n",
    "        \n",
    "        # Create separate columns for different entity types\n",
    "        self.df['persons'] = self.df['named_entities'].apply(\n",
    "            lambda entities: [e['word'] for e in entities if e['entity'] == 'I-PER']\n",
    "        )\n",
    "        self.df['locations'] = self.df['named_entities'].apply(\n",
    "            lambda entities: [e['word'] for e in entities if e['entity'] == 'I-LOC']\n",
    "        )\n",
    "        \n",
    "        return self.df[['persons', 'locations']]\n",
    "    \n",
    "    def preprocess_data(self, text_column='Abstract_tokens', year_column='Year'):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with XLM-RoBERTa tokenization\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        self.df[text_column] = self.df[text_column].astype(str).str.lower()\n",
    "        \n",
    "        # Tokenize using XLM-RoBERTa tokenizer\n",
    "        encoded_inputs = self.ner_tokenizer(\n",
    "            self.df[text_column].tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Set max length and vocab size\n",
    "        self.max_length = encoded_inputs['input_ids'].shape[1]\n",
    "        self.vocab_size = self.ner_tokenizer.vocab_size\n",
    "        \n",
    "        # Encode publication years\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df['year_encoded'] = label_encoder.fit_transform(self.df[year_column])\n",
    "        \n",
    "        return encoded_inputs, self.df['year_encoded']\n",
    "    \n",
    "    def create_transformer_model(self, embedding_dim=128, num_heads=8, ff_dim=32):\n",
    "        \"\"\"\n",
    "        Create a Transformer-based model with XLM-RoBERTa inspired architecture\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.Input(shape=(self.max_length,))\n",
    "        \n",
    "        # Use pre-trained embedding layer concept from XLM-RoBERTa\n",
    "        x = tf.keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size, \n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        # Multi-head attention inspired layers\n",
    "        x = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embedding_dim\n",
    "        )(x, x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X, y, test_size=0.2, epochs=50):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X['input_ids'], y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        model = self.create_transformer_model()\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "\n",
    "# Example usage\n",
    "analyzer = PsychologyResearchAnalyzer('articles_tokenize.csv')\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = analyzer.extract_named_entities()\n",
    "\n",
    "# Preprocess data\n",
    "X, y = analyzer.preprocess_data()\n",
    "\n",
    "# Train model\n",
    "model, training_history = analyzer.train_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b22ad-d8e1-43af-8e56-dabf4975b6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebecab-6a7d-4dd8-a9f3-5663b89678ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class PsychologyResearchAnalyzer:\n",
    "    def __init__(self, data_path):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Load XLM-RoBERTa NER model and tokenizer\n",
    "        self.ner_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "        self.ner_pipeline = pipeline(\"ner\", model=self.ner_model, tokenizer=self.ner_tokenizer)\n",
    "        \n",
    "        # Additional attributes for ML processing\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "    \n",
    "    def preprocess_data(self, text_column='Abstract', year_column='Year'):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing with XLM-RoBERTa tokenization\n",
    "        \"\"\"\n",
    "        # Clean text\n",
    "        self.df[text_column] = self.df[text_column].astype(str).str.lower()\n",
    "        \n",
    "        # Tokenize using XLM-RoBERTa tokenizer\n",
    "        encoded_inputs = self.ner_tokenizer(\n",
    "            self.df[text_column].tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        # Set max length and vocab size\n",
    "        self.max_length = encoded_inputs['input_ids'].shape[1]\n",
    "        self.vocab_size = self.ner_tokenizer.vocab_size\n",
    "        \n",
    "        # Encode publication years\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.df['year_encoded'] = label_encoder.fit_transform(self.df[year_column])\n",
    "        \n",
    "        return encoded_inputs, self.df['year_encoded']\n",
    "    \n",
    "    def create_transformer_model(self, embedding_dim=128, num_heads=8, ff_dim=32):\n",
    "        \"\"\"\n",
    "        Create a Transformer-based model with XLM-RoBERTa inspired architecture\n",
    "        \"\"\"\n",
    "        inputs = tf.keras.Input(shape=(self.max_length,))\n",
    "        \n",
    "        x = tf.keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size, \n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_length\n",
    "        )(inputs)\n",
    "        \n",
    "        x = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embedding_dim\n",
    "        )(x, x)\n",
    "        \n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(ff_dim//2, activation='relu')(x)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def cross_validate(self, X, y, n_splits=5, epochs=50):\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation with comprehensive metrics\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        input_ids = X['input_ids']\n",
    "        \n",
    "        # Initialize cross-validation\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Metrics storage\n",
    "        cv_scores = {\n",
    "            'mse': [],\n",
    "            'r2': []\n",
    "        }\n",
    "        \n",
    "        # Fold-wise model training and evaluation\n",
    "        for fold, (train_indices, val_indices) in enumerate(kfold.split(input_ids), 1):\n",
    "            # Split data\n",
    "            X_train, X_val = input_ids[train_indices], input_ids[val_indices]\n",
    "            y_train, y_val = y[train_indices], y[val_indices]\n",
    "            \n",
    "            # Create and train model\n",
    "            model = self.create_transformer_model()\n",
    "            model.fit(\n",
    "                X_train, y_train, \n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(X_val).flatten()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            \n",
    "            # Store metrics\n",
    "            cv_scores['mse'].append(mse)\n",
    "            cv_scores['r2'].append(r2)\n",
    "            \n",
    "            print(f\"Fold {fold}: MSE = {mse:.4f}, R² = {r2:.4f}\")\n",
    "        \n",
    "        # Compute average cross-validation scores\n",
    "        avg_mse = np.mean(cv_scores['mse'])\n",
    "        avg_r2 = np.mean(cv_scores['r2'])\n",
    "        \n",
    "        print(\"\\nCross-Validation Results:\")\n",
    "        print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "        print(f\"Average R²: {avg_r2:.4f}\")\n",
    "        \n",
    "        return cv_scores\n",
    "\n",
    "# Example usage\n",
    "analyzer = PsychologyResearchAnalyzer('articles_tokenize.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X, y = analyzer.preprocess_data()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = analyzer.cross_validate(X, y, n_splits=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
