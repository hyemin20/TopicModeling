{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eac5082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hyemi\\\\Python\\\\TopicModeling'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007a03a",
   "metadata": {},
   "source": [
    "# Frontier in Psychology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712807f3-b4f8-481c-a49d-0279316e9353",
   "metadata": {},
   "source": [
    "# 1.background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdefa19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6164f5",
   "metadata": {},
   "source": [
    "##### Bring URL and HTML\n",
    "url = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3bcdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Convert BeautifulSoup object to string\n",
    "html_content = str(soup)\n",
    "\n",
    "##### Search for title pattern\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html_content, re.IGNORECASE)\n",
    "title = match_results.group() if match_results else \"No title found\"\n",
    "title = re.sub(\"<.*?>\", \"\", title)  # Remove HTML tags\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e360e6",
   "metadata": {},
   "source": [
    "##### https://www.frontiersin.org/journals/psychology/articles?publication-date=25%2F10%2F2024-25%2F10%2F2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6af844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles loaded: 128, Retries: 0\n",
      "Articles loaded: 128, Retries: 1\n",
      "Articles loaded: 128, Retries: 2\n",
      "Articles loaded: 128, Retries: 3\n",
      "Articles loaded: 128, Retries: 4\n",
      "Articles loaded: 128, Retries: 5\n",
      "Articles loaded: 128, Retries: 6\n",
      "Articles loaded: 128, Retries: 7\n",
      "Articles loaded: 128, Retries: 8\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Open the page\n",
    "base_url = \"https://www.frontiersin.org/journals/psychology/articles\"\n",
    "url = base_url + \"?publication-date=01%2F01%2F2010-31%2F12%2F2010\"\n",
    "driver.get(url)\n",
    "\n",
    "# Set page zoom to 25%\n",
    "driver.execute_script(\"document.body.style.zoom='25%'\")\n",
    "\n",
    "# Accept cookies if the banner appears\n",
    "try:\n",
    "    accept_cookies_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "    )\n",
    "    accept_cookies_button.click()\n",
    "except:\n",
    "    print(\"No cookie banner detected.\")\n",
    "\n",
    "# Wait for the first article to load\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, \"CardArticle\"))\n",
    ")\n",
    "\n",
    "# Scroll and hover to trigger loading of new articles\n",
    "scroll_increment = 200\n",
    "scroll_pause_time = 1.5\n",
    "max_retries = 8\n",
    "retries = 0\n",
    "prev_article_count = 0\n",
    "\n",
    "# Scroll until all articles are loaded\n",
    "actions = ActionChains(driver)\n",
    "while retries < max_retries:\n",
    "    # Scroll by a small increment\n",
    "    driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "    time.sleep(scroll_pause_time)\n",
    "    \n",
    "    # Hover over the footer to trigger lazy loading\n",
    "    footer = driver.find_element(By.TAG_NAME, \"footer\")\n",
    "    actions.move_to_element(footer).perform()\n",
    "    \n",
    "    # Check current number of articles loaded\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all(\"article\", class_=\"CardArticle\")\n",
    "    current_article_count = len(articles)\n",
    "    \n",
    "    # Update retry count based on loaded articles\n",
    "    if current_article_count > prev_article_count:\n",
    "        prev_article_count = current_article_count\n",
    "        retries = 0  # Reset retries if new articles loaded\n",
    "    else:\n",
    "        retries += 1  # Increment retry count if no new articles load\n",
    "    print(f\"Articles loaded: {current_article_count}, Retries: {retries}\")\n",
    "\n",
    "# Ensure elements are fully loaded\n",
    "time.sleep(3)\n",
    "\n",
    "# Extract article information with error handling\n",
    "data = []\n",
    "for article in articles:\n",
    "    try:\n",
    "        # Find elements with None handling\n",
    "        title_elem = article.find(\"h1\", class_=\"CardArticle__title\")\n",
    "        date_elem = article.find(\"p\", class_=\"CardArticle__date\")\n",
    "        type_elem = article.find(\"p\", class_=\"CardArticle__type\")\n",
    "        link_elem = article.find(\"a\", class_=\"CardArticle__wrapper\")\n",
    "        \n",
    "        # Extract data with None checks\n",
    "        title = title_elem.get_text(strip=True) if title_elem else None\n",
    "        publish_date = date_elem.get_text(strip=True) if date_elem else None\n",
    "        article_type = type_elem.get_text(strip=True) if type_elem else None\n",
    "        link = link_elem[\"href\"] if link_elem else None\n",
    "        \n",
    "        data.append({\n",
    "            \"Title\": title,\n",
    "            \"Type\": article_type,\n",
    "            \"Published Date\": publish_date,\n",
    "            \"Link\": link,\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159e75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and process the data\n",
    "def clean_publication_dates(df):\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Clean date strings and convert to datetime\n",
    "    df['Published Date'] = df['Published Date'].str.replace(\n",
    "        r'(Published on |Accepted on )', '', \n",
    "        regex=True\n",
    "    )\n",
    "    \n",
    "    # Convert to datetime with proper error handling\n",
    "    df['Published Date'] = pd.to_datetime(\n",
    "        df['Published Date'],\n",
    "        dayfirst=True,  # Assuming date format is DD/MM/YYYY\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Extract year and handle missing values\n",
    "    df['Year'] = df['Published Date'].dt.strftime('%Y')\n",
    "    df['Year'] = df['Year'].fillna('2010')  # Fill missing years with 2014\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_research_articles(df):\n",
    "    # Filter for Original Research articles\n",
    "    research_df = df[df['Type'] == 'Original Research'].copy()\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    research_df = research_df.reset_index(drop=True)\n",
    "    \n",
    "    return research_df\n",
    "\n",
    "# Process the dataframe\n",
    "processed_df = clean_publication_dates(df)\n",
    "\n",
    "# Filter for Original Research articles\n",
    "research_df = filter_research_articles(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7932307c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1487146/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfeac4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da64b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bda5c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#link = \"https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1435688/full\"\n",
    "#response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#paragraphs = soup.find_all('p')\n",
    "#paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2390d8a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "#abstract = abstract_meta['content']\n",
    "#abstract_text = re.search(r'<p>(.*?)</p>', abstract).group(1)\n",
    "#abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1994fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections = [\"Data availability statement\", \"Ethics statement\", \"Author contributions\"]\n",
    "#extracted_text = []\n",
    "#found_section = False\n",
    "\n",
    "# Traverse paragraphs until we reach the first target section\n",
    "#for paragraph in soup.find_all(['p', 'h2']):\n",
    "    # If we encounter any target section, stop the extraction\n",
    "#    if paragraph.name == 'h2' and paragraph.text.strip() in sections:\n",
    "#        found_section = True\n",
    "#        break\n",
    "#    elif paragraph.name == 'p':  # Accumulate text in paragraphs\n",
    "#        extracted_text.append(paragraph.text.strip())\n",
    "\n",
    "# Join and print the extracted text\n",
    "#result = \" \".join(extracted_text)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce65f4a-4f71-466b-8c12-4ea6a19a7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during processing: name 'Retry' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Retry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m research_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m research_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m research_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired columns (Title, Year, Link) not found in research_df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal articles processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 52\u001b[0m, in \u001b[0;36mprocess_articles\u001b[1;34m(research_df)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_articles\u001b[39m(research_df):\n\u001b[1;32m---> 52\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m     total_articles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(research_df)\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mcreate_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_session\u001b[39m():\n\u001b[0;32m      2\u001b[0m     session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[1;32m----> 3\u001b[0m     retries \u001b[38;5;241m=\u001b[39m \u001b[43mRetry\u001b[49m(\n\u001b[0;32m      4\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      5\u001b[0m         backoff_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      6\u001b[0m         status_forcelist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m403\u001b[39m, \u001b[38;5;241m408\u001b[39m, \u001b[38;5;241m429\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m502\u001b[39m, \u001b[38;5;241m503\u001b[39m, \u001b[38;5;241m504\u001b[39m]\n\u001b[0;32m      7\u001b[0m     )\n\u001b[0;32m      8\u001b[0m     adapter \u001b[38;5;241m=\u001b[39m HTTPAdapter(max_retries\u001b[38;5;241m=\u001b[39mretries)\n\u001b[0;32m      9\u001b[0m     session\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m\"\u001b[39m, adapter)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Retry' is not defined"
     ]
    }
   ],
   "source": [
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[403, 408, 429, 500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    return session\n",
    "\n",
    "def extract_article_details(session, url, retries=3):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            ## Extract authors\n",
    "            #authors = soup.select('.authors .author-wrapper')\n",
    "            #author_list = [author.get_text(strip=True) for author in authors]\n",
    "            \n",
    "            # Extract abstract\n",
    "            abstract_meta = soup.find('meta', attrs={'name': 'citation_abstract'})\n",
    "            abstract = abstract_meta['content'] if abstract_meta else \"Abstract not available\"\n",
    "            \n",
    "            # Extract title from meta tag (more reliable)\n",
    "            title_meta = soup.find('meta', attrs={'name': 'citation_title'})\n",
    "            title = title_meta['content'] if title_meta else \"Title not available\"\n",
    "            \n",
    "            return {\n",
    "                'Title': title,\n",
    "                #'Authors': '; '.join(author_list) if author_list else \"Authors not available\",\n",
    "                'Abstract': abstract\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                return {\n",
    "                    'Title': \"Error extracting title\",\n",
    "                    #'Authors': \"Error extracting authors\",\n",
    "                    'Abstract': \"Error extracting abstract\"\n",
    "                }\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "def process_articles(research_df):\n",
    "    session = create_session()\n",
    "    processed_data = []\n",
    "    total_articles = len(research_df)\n",
    "    \n",
    "    for idx, row in research_df.iterrows():\n",
    "        print(f\"Processing article {idx + 1}/{total_articles}\")\n",
    "        \n",
    "        article_data = {\n",
    "            'Title': row['Title'],\n",
    "            'Year': row['Year'],\n",
    "            'Link': row['Link']\n",
    "        }\n",
    "        \n",
    "        if row['Link']:\n",
    "            details = extract_article_details(session, row['Link'])\n",
    "            article_data.update({\n",
    "                #'Authors': details['Authors'],\n",
    "                'Abstract': details['Abstract']\n",
    "            })\n",
    "        else:\n",
    "            article_data.update({\n",
    "                #'Authors': \"Link not available\",\n",
    "                'Abstract': \"Link not available\"\n",
    "            })\n",
    "        \n",
    "        processed_data.append(article_data)\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Process the articles and create final dataset\n",
    "try:\n",
    "    if 'Title' not in research_df.columns or 'Year' not in research_df.columns or 'Link' not in research_df.columns:\n",
    "        raise ValueError(\"Required columns (Title, Year, Link) not found in research_df\")\n",
    "    \n",
    "    final_df = process_articles(research_df)\n",
    "    \n",
    "    print(\"\\nProcessing completed!\")\n",
    "    print(f\"Total articles processed: {len(final_df)}\")\n",
    "    #print(f\"Articles with authors: {final_df['Authors'].notna().sum()}\")\n",
    "    print(f\"Articles with abstracts: {final_df['Abstract'].notna().sum()}\")\n",
    "\n",
    "    # Display DataFrame using IPython display (for Jupyter)\n",
    "    from IPython.display import display\n",
    "    display(final_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf499f89-6246-4173-8666-f8ae888f5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = final_df\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "        \n",
    "    # Remove HTML tags (including <p> and </p>)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove parenthetical content\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    \n",
    "    # Remove numbers, asterisks, and daggers\n",
    "    text = re.sub(r'(\\d+|\\*|†)', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Clean Title and Abstract\n",
    "#result_df['Title'] = result_df['Title'].apply(clean_text)\n",
    "result_df['Abstract'] = result_df['Abstract'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e2902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df['Authors'] = result_df['Authors'].apply(\n",
    "#    lambda x: re.sub(r'(\\d+|\\*|†)', '', x)  # Remove digits, asterisks, and dagger symbols\n",
    "#                .replace(\"??\", \"\")          # Remove any question marks\n",
    "#                .replace(\",,\", \",\")         # Consolidate multiple commas\n",
    "#                .replace(\", ,\", \",\")        # Remove spaced commas\n",
    "#                .replace(\"  \", \" \")         # Remove double spaces\n",
    "#                .replace(\", ,\", \",\")                # Remove any trailing commas or spaces\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7545a69-5c3b-4dd1-a78a-e229a5a31c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df[['Year', 'Abstract']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"articles_data_10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5437233",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5437fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
