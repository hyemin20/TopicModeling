{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22071307",
   "metadata": {},
   "source": [
    "time to start think the research question \\\n",
    "ex1. What are the main keywords in this journal, and how are they related? \\\n",
    "ex2. How have keyword trends in this journal changed over time? \\\n",
    "-> need to decide how to extract keyword. by word frequency? tf-idf? \\\n",
    "-> can the keyword be clustered? \\\n",
    "   or can we find clustering distribution of the articles then find keywords for each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a0741f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_extra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn_extra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMedoids\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m### tqdm\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_extra'"
     ]
    }
   ],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' IMPORT LIBRARIES\n",
    "###'\n",
    "###'\n",
    "\n",
    "### pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import spacy\n",
    "\n",
    "### punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spellchecker import SpellChecker\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import scattertext as st\n",
    "import re\n",
    "\n",
    "### textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "### countvectorizer, tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "### tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "### gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "### PCA\n",
    "import random\n",
    "from adjustText import adjust_text\n",
    "\n",
    "### plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### kMeans and silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "### ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef95c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\OWNER\\\\TopicModeling'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6340b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# ----------\n",
    "\n",
    "\n",
    "df_14 = pd.read_csv(\"articles_data_14.csv\")\n",
    "df_24 = pd.read_csv(\"articles_data_24.csv\")\n",
    "    \n",
    "df_14['Published'] = 0  # Code 0 for data from df_14\n",
    "df_24['Published'] = 1  # Code 1 for data from df_24\n",
    "\n",
    "df_14 = df_14[df_14['Abstract'] != 'Abstract not available.']\n",
    "df_24 = df_24[df_24['Abstract'] != 'Abstract not available.']\n",
    "\n",
    "\n",
    "df_14_S = df_14[['Title','Abstract','Published','Full Text']].dropna()\n",
    "df_14_S['Abstract'] = df_14_S['Abstract'].apply(lambda x: re.sub(r'<\\d+|\\*|†>', '', re.sub(r'\\<.*?\\>', '', x)))\n",
    "\n",
    "df_24_S = df_24[['Title','Abstract','Published','Full Text']].dropna()\n",
    "df_24_S['Abstract'] = df_24_S['Abstract'].apply(lambda x: re.sub(r'<\\d+|\\*|†>', '', re.sub(r'\\<.*?\\>', '', x)))\n",
    "\n",
    "#df_S.head()\n",
    "#len(df_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468625dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Abstract'] = df['Abstract'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af695be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "###' ################################################################################\n",
    "###'\n",
    "###' Function for Deleteing Punctuations and StopWords\n",
    "###'\n",
    "###'\n",
    "\n",
    "### define fuction\n",
    "def rem_punc_stop(text):\n",
    "    # When text is None\n",
    "    if text is None:\n",
    "        return []\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Define additional stop words\n",
    "    stop_words = STOP_WORDS | {\"abstract\", \"available\", \"student\", \"research\", \"study\", \"impact\", \"effect\",\n",
    "                               \"result\", \"al\", \"et\", \"doi\", \"googlescholar\", \"google\", \"scholar\", \"textgoogle\", \n",
    "                               \"full\", \"crossref\", \"introduction\", \"background\", \"purpose\" \"aim\", \"objective\",\"use\",\"child\"}\n",
    "    # Define punctuation\n",
    "    punc = set(punctuation)\n",
    "\n",
    "    # Remove punctuation\n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "\n",
    "    # Apply NLP processing\n",
    "    doc = nlp(punc_free)\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    text_lemma = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "    # Filter tokens to remove URLs, stop words, and non-alphabetic tokens\n",
    "    filtered_tokens = [word for word in text_lemma.split() if word not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Return filtered tokens for TfidfVectorizer\n",
    "    return filtered_tokens\n",
    "    \n",
    "\n",
    "###' ################################################################################\n",
    "###'\n",
    "###' Apply the Function and Tokenize Text Column\n",
    "###'\n",
    "###'\n",
    "\n",
    "### sample from the whole dataset\n",
    "df_24_S['Title_tokens'] = df_24_S['Title'].map(lambda x: rem_punc_stop(x))\n",
    "df_24_S['Title_join'] = df_24_S['Title_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "df_24_S['Abstract_tokens'] = df_24_S['Abstract'].map(lambda x: rem_punc_stop(x))\n",
    "df_24_S['Abstract_join'] = df_24_S['Abstract_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "\n",
    "#df_24_S['Full_tokens'] = df_24_S['Full Text'].map(lambda x: rem_punc_stop(x))\n",
    "#df_24_S['Full_join'] = df_24_S['Full_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "\n",
    "###' ################################################################################\n",
    "###'\n",
    "###' Filter Yet Published Papers\n",
    "###'\n",
    "###'\n",
    "\n",
    "#df_S['Full_count'] = df_S['Full Text'].dropna().apply(lambda x: len(str(x).split()))\n",
    "#df_S = df_S[df_S['Full_count'] >= 2000]\n",
    "\n",
    "df_S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af216180",
   "metadata": {},
   "source": [
    "# 3. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131b0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a219811",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_24_S['Full Text']\n",
    "tf = TfidfVectorizer(tokenizer = rem_punc_stop,  # specify our function for remove punc and stop words\n",
    "                     token_pattern = None)       # specify \"None\" to remove warning. Is this necessary?\n",
    "tfidf_matrix =  tf.fit_transform(X)\n",
    "\n",
    "# modify the output to be a dense matrix\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff889849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(data = tfidf_matrix.toarray(),       # convert to array than to datafram\n",
    "                        columns=tf.get_feature_names_out())\n",
    "# sort by term frequency on the first document\n",
    "tfidf_df.T.nlargest(10,  # transpose the matrix = columns become documents and rows are words\n",
    "                     0)  # on column index 0 to show the largest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd391b",
   "metadata": {},
   "source": [
    "# 3.1 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61597ec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "for i in tqdm(range(2, 11)):\n",
    "    kmedoids = KMedoids(n_clusters=i, random_state=42).fit(tfidf_matrix)\n",
    "    #inertia가 군집 내의 분산을 의미\n",
    "    distortions.append(kmedoids.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(2, 11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n",
    "\n",
    "### 4 > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# determining optional k: silhouette scores\n",
    "# ------------------------------\n",
    "# define a number of functions\n",
    "\n",
    "# iterate over a k-means fits to have different clusters\n",
    "# ---------\n",
    "def run_KMedoids(max_k, data):\n",
    "    max_k += 1\n",
    "    kmedoids_results = dict()\n",
    "    for k in range(2 , max_k):\n",
    "        kmedoids = KMedoids(n_clusters=k,\n",
    "                            random_state=42)\n",
    "        kmedoids_results.update( {k : kmedoids.fit(data)} )\n",
    "        \n",
    "    return kmedoids_results\n",
    "\n",
    "\n",
    "# calculate average silhouettes scores \n",
    "# ---------\n",
    "# plot silhouettes scores\n",
    "def printAvg(avg_dict):\n",
    "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
    "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
    "\n",
    "# plot silhouettes scores       \n",
    "def plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg): \n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax1.set_xlim([-0.2, 1])   # play with this to set x-axis limits\n",
    "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.title((\"Silhouette analysis for K = %d\" % n_clusters), fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = 10\n",
    "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(numpy.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
    "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
    "    plt.show()\n",
    "    \n",
    "# put it altogether\n",
    "def silhouette(kmeans_dict, df, plot=True):\n",
    "    df = df.to_numpy()\n",
    "    avg_dict = dict()\n",
    "    for n_clusters, kmeans in kmeans_dict.items():      \n",
    "        kmeans_labels = kmeans.predict(df)\n",
    "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
    "        avg_dict.update( {silhouette_avg : n_clusters} )\n",
    "    \n",
    "        if(plot): plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7 # choose 8 based on the elbow method result from above\n",
    "\n",
    "# run the k-means algorithm\n",
    "kmedoids_results = run_KMedoids(k, data = tfidf_df) \n",
    "\n",
    "# plot the silhouette analysis\n",
    "silhouette(kmedoids_results, tfidf_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top features from each cluster\n",
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = numpy.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = numpy.where(prediction==label) # indices for each cluster\n",
    "        x_means = numpy.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = numpy.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = tf.get_feature_names_out()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "# plot them on a barplot\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = 7\n",
    "kmedoids = kmedoids_results.get(best_result)\n",
    "\n",
    "# processing for plot\n",
    "tfidf_array = tfidf_df.to_numpy()     # convert dataframe to array\n",
    "prediction = kmedoids.predict(tfidf_df) # predict cluster using tf-idf dataframe\n",
    "\n",
    "\n",
    "# plot\n",
    "n_feats = 20\n",
    "dfs = get_top_features_cluster(tfidf_array, # specify dataset which is an array\n",
    "                               prediction,  # make specify prediciton\n",
    "                               n_feats )    # set number of features \n",
    "plotWords(dfs, # specify data for plotting  \n",
    "          13)  # set number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bbfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212ebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c19f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "\n",
    "K = range(2, 11) \n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k,\n",
    "                random_state=42, # number of clusters\n",
    "                init='k-means++', # method for initalization \n",
    "                n_init=10)     # number of times the k-means algorithm is run with different centroid seeds\n",
    "    km.fit(tfidf_matrix)     # fit\n",
    "    inertia.append(km.inertia_) # pipe inertia calculations into list\n",
    "\n",
    "\n",
    "# plot results\n",
    "# ---------\n",
    "plt.plot(K, inertia, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.xticks(range(1, max(K) + 1, 1))\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "### 7 or 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# determining optional k: silhouette scores\n",
    "# ------------------------------\n",
    "# define a number of functions\n",
    "\n",
    "# iterate over a k-means fits to have different clusters\n",
    "# ---------\n",
    "def run_KMeans(max_k, data):\n",
    "    max_k += 1\n",
    "    kmeans_results = dict()\n",
    "    for k in range(2 , max_k):\n",
    "        kmeans = KMeans(n_clusters = k\n",
    "                               , init = 'k-means++'\n",
    "                               , n_init = 10\n",
    "                               , random_state = 42)\n",
    "\n",
    "        kmeans_results.update( {k : kmeans.fit(data)} )\n",
    "        \n",
    "    return kmeans_results\n",
    "\n",
    "\n",
    "# calculate average silhouettes scores \n",
    "# ---------\n",
    "# plot silhouettes scores\n",
    "def printAvg(avg_dict):\n",
    "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
    "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
    "\n",
    "# plot silhouettes scores       \n",
    "def plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg): \n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax1.set_xlim([-0.2, 1])   # play with this to set x-axis limits\n",
    "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.title((\"Silhouette analysis for K = %d\" % n_clusters), fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = 10\n",
    "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(numpy.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
    "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
    "    plt.show()\n",
    "    \n",
    "# put it altogether\n",
    "def silhouette(kmeans_dict, df, plot=True):\n",
    "    df = df.to_numpy()\n",
    "    avg_dict = dict()\n",
    "    for n_clusters, kmeans in kmeans_dict.items():      \n",
    "        kmeans_labels = kmeans.predict(df)\n",
    "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
    "        avg_dict.update( {silhouette_avg : n_clusters} )\n",
    "    \n",
    "        if(plot): plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7 # choose 8 based on the elbow method result from above\n",
    "\n",
    "# run the k-means algorithm\n",
    "kmeans_results = run_KMeans(k,                # set k\n",
    "                            data = tfidf_df)  # identify data\n",
    "\n",
    "\n",
    "# plot the silhouette analysis\n",
    "silhouette(kmeans_results,     # take k-means results\n",
    "           tfidf_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c502895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top features from each cluster\n",
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = numpy.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = numpy.where(prediction==label) # indices for each cluster\n",
    "        x_means = numpy.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = numpy.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = tf.get_feature_names_out()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "# plot them on a barplot\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = 7\n",
    "kmeans = kmeans_results.get(best_result)\n",
    "\n",
    "# processing for plot\n",
    "tfidf_array = tfidf_df.to_numpy()     # convert dataframe to array\n",
    "prediction = kmeans.predict(tfidf_df) # predict cluster using tf-idf dataframe\n",
    "\n",
    "\n",
    "# plot\n",
    "n_feats = 20\n",
    "dfs = get_top_features_cluster(tfidf_array, # specify dataset which is an array\n",
    "                               prediction,  # make specify prediciton\n",
    "                               n_feats )    # set number of features \n",
    "plotWords(dfs, # specify data for plotting  \n",
    "          13)  # set number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4509a77",
   "metadata": {},
   "source": [
    "# 3.2 Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' Topic Modeling : Latent Dirichlet Allocation (LDA)\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### define function\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nlda_{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Latent Dirichlet Allocation (LDA)\n",
    "# ---------------------------------------\n",
    "\n",
    "# pre-processing\n",
    "# --------\n",
    "# create a new data object called X\n",
    "X = df_S['Title_join']\n",
    "tf = TfidfVectorizer(tokenizer = rem_punc_stop,  # specify our function for remove punc and stop words\n",
    "                     token_pattern = None)       # specify \"None\" to remove warning. Is this necessary?\n",
    "\n",
    "# apply tf-idf vectorizer to our data (X)\n",
    "tfidf_matrix =  tf.fit_transform(X)\n",
    "\n",
    "# modify the output to be a dense matrix\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# intitialize LDA model and \n",
    "# --------\n",
    "# initialize LDA and set model parameters\n",
    "lda = LatentDirichletAllocation(n_components=7, # specify the number of components\n",
    "                                max_iter=20,    # specify the number of iterations \n",
    "                                random_state=42) # set a seed for reproducibility\n",
    "\n",
    "# fit LDA model to our dense matrix\n",
    "lda = lda.fit(numpy.asarray(dense_matrix))\n",
    "\n",
    "# post-processing\n",
    "# --------\n",
    "# get feature names from our tf-idf vector\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# print top words \n",
    "print_top_words(lda,               # specify model\n",
    "                tf_feature_names,  # specify feature names vector\n",
    "                20)                # specify how many words we want to see\n",
    " \n",
    "\n",
    "# now transform our data using the lda model and create a dataframe\n",
    "col_names = ['lda_0', 'lda_1','lda_2', 'lda_3','lda_4','lda_5', 'lda_6']\n",
    "topic_dist = lda.transform(tfidf_matrix)\n",
    "topic_dist_df = pd.DataFrame(topic_dist, columns = col_names).reset_index(drop = True)\n",
    "\n",
    "# view the corresponding tf-idf dataframe with tf-idf values\n",
    "topic_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_topics = df_S.join(topic_dist_df.reset_index())\n",
    "df_w_topics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc0835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144264fd",
   "metadata": {},
   "source": [
    "# 3.3 Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' Topic Modeling : on-Negative Matrix Factorization (NMF)\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### define function\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nnmf_{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "# pre-processing\n",
    "Y = df_S['Abstract_join']\n",
    "\n",
    "tf = TfidfVectorizer(tokenizer = rem_punc_stop,\n",
    "                     token_pattern = None) \n",
    "\n",
    "tfidf_mx =  tf.fit_transform(Y)\n",
    "dense_matrix = tfidf_mx.todense()\n",
    "\n",
    "\n",
    "# initialize LDA and set model parameters\n",
    "nmf = NMF(n_components=7,  # specify the number of components\n",
    "          init='random',   # specify the initalization method\n",
    "          random_state=42)  # set a seed for reproducibility\n",
    "\n",
    "# fit NMF model to our dense matrix\n",
    "nmf = nmf.fit(numpy.asarray(dense_matrix))\n",
    "\n",
    "# post-processing\n",
    "tf_feature_names = tf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "### print top words \n",
    "print_top_words(nmf, tf_feature_names,20)  \n",
    "\n",
    "### join\n",
    "col_names2 = ['nmf_0', 'nmf_1', 'nmf_2', 'nmf_3', 'nmf_4', 'nmf_5', 'nmf_6']\n",
    "topic_dist2 = nmf.transform(tfidf_mx)\n",
    "topic_dist_nmf = pd.DataFrame(topic_dist2, columns = col_names2).reset_index(drop = True)\n",
    "topic_dist_nmf\n",
    "\n",
    "#df_w_topics2 = topic_dist_df.join(topic_dist_nmf.reset_index())\n",
    "#df_w_topics2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05589ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49005c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_train = pca.fit_transform(df_24_S['Abstract'])\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f759f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd23c25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fe774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
