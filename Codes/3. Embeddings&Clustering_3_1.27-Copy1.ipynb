{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb08d34f-510a-4eec-b49e-c0cbb2c18a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' IMPORT LIBRARIES\n",
    "###'\n",
    "###'\n",
    "\n",
    "### pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "### punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spellchecker import SpellChecker\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import scattertext as st\n",
    "import re\n",
    "\n",
    "### textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08edcb63-8ab4-4e39-9a41-22865e2c713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hyemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' IMPORT LIBRARIES\n",
    "###'\n",
    "###'\n",
    "\n",
    "### countvectorizer, tfidfvectorizer, embeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel, Trainer, TrainingArguments, pipeline\n",
    "import umap\n",
    "\n",
    "### gpu\n",
    "#device = torch.device(\"npu:0\") \n",
    "#tensor = torch.randn(3, 3).to(device) \n",
    "#print(tensor.device)\n",
    "# 출력: npu:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03bcf177-31f3-47cb-ac88-b5c71cf8ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' IMPORT LIBRARIES\n",
    "###'\n",
    "###'\n",
    "\n",
    "### tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "### gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "### PCA\n",
    "import random\n",
    "from adjustText import adjust_text\n",
    "\n",
    "### plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### kMeans and silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "### ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbe3a0-afb9-4088-8942-22b1e1994e4e",
   "metadata": {},
   "source": [
    "## 0. Embeddings Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04ae0e1-cf68-45bd-a7df-bf1d9cc2f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = Path(r\"C:\\Users\\Hyemi\\Python\\TopicModeling\\Data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "file_path = data_dir / \"articles_tokenize.csv\"\n",
    "articles_full = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534bfbc-97c9-40ac-a767-7772b633997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify journals into groups\n",
    "def classify_year_group(year):\n",
    "    if 2013 <= year <= 2016:\n",
    "        return \"1\"\n",
    "    elif 2017 <= year <= 2019:\n",
    "        return \"2\"\n",
    "    elif 2020 <= year <= 2021:\n",
    "        return \"3\"\n",
    "    elif 2022 <= year <= 2024:\n",
    "        return \"4\"\n",
    "    else:\n",
    "        return \"5\"\n",
    "\n",
    "articles_full[\"Year_Group\"] = articles_full[\"Year\"].apply(classify_year_group)\n",
    "\n",
    "# Load XLM-RoBERTa model and tokenizer\n",
    "model_name = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6784ec-24b3-4f69-bfdc-c46905eb3323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get_embeddings(texts, model, tokenizer, batch_size=32):\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                           return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract CLS token representations\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Combine all batch embeddings\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e98244-cd87-462f-8844-42681ce23221",
   "metadata": {},
   "source": [
    "## 3. Embedding: group3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f75e48-0f6c-4a51-a1f2-bf1ee7b1da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# Modify the column assignment\n",
    "df = articles_full[articles_full[\"Year_Group\"] == \"3\"]\n",
    "df[\"Embeddings\"] = list(batch_get_embeddings(\n",
    "    df[\"Abstract\"].tolist(), \n",
    "    model, \n",
    "    tokenizer\n",
    "))\n",
    "\n",
    "# Rest of the clustering code remains the same\n",
    "embeddings_matrix = np.vstack(df[\"Embeddings\"].values)\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_model.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Optional: Add progress tracking\n",
    "print(\"Embedding generation complete. Shape:\", embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd2341-fd0a-4edc-b1ae-50417d3dc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Within-Cluster Sum of Squares (WCSS) for different cluster sizes\n",
    "def plot_elbow_method(embeddings_matrix, max_clusters=9):\n",
    "    wcss = []  # List to store WCSS for each number of clusters\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(embeddings_matrix)\n",
    "        wcss.append(kmeans.inertia_)  # Inertia is the sum of squared distances to the closest centroid\n",
    "    \n",
    "    # Plot Elbow Graph\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(2, max_clusters + 1), wcss, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n",
    "    plt.title(\"Elbow Method for Optimal k\")\n",
    "    plt.xticks(range(1, max_clusters + 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# Run the Elbow Method function to determine the optimal number of clusters\n",
    "plot_elbow_method(embeddings_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669a3c1-5086-47ee-83c1-2f2cbc1d3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings_2d)\n",
    "\n",
    "# Add clustering results to the dataframe\n",
    "df[\"Cluster\"] = clusters\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], hue=df[\"Cluster\"], palette=\"tab10\", alpha=0.6)\n",
    "plt.title(\"Word Clusters Based on Abstract Embeddings\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b1dac-439d-4285-a850-3420c8dc8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "umap_3d = umap.UMAP(n_components=3, random_state=42)\n",
    "embeddings_3d = umap_3d.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Set number of clusters\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings_3d)  # Use 3D embeddings\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(\n",
    "    embeddings_3d[:, 0], embeddings_3d[:, 1], embeddings_3d[:, 2], \n",
    "    c=df[\"Cluster\"], cmap=\"tab10\", alpha=0.6\n",
    ")\n",
    "\n",
    "# Labels and Title\n",
    "ax.set_title(\"Word Clusters Based on Abstract Embeddings (3D)\")\n",
    "ax.set_xlabel(\"UMAP Dimension 1\")\n",
    "ax.set_ylabel(\"UMAP Dimension 2\")\n",
    "ax.set_zlabel(\"UMAP Dimension 3\")\n",
    "\n",
    "# Add legend\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b09e3-fb83-4a8d-957d-419c17379ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(r\"C:\\Users\\Hyemi\\Python\\TopicModeling\\Data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = data_dir / \"articles_embedding_3_full.csv\"\n",
    "\n",
    "df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
