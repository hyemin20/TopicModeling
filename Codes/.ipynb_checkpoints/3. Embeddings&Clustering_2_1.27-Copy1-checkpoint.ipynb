{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb08d34f-510a-4eec-b49e-c0cbb2c18a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hyemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' IMPORT LIBRARIES\n",
    "###'\n",
    "###'\n",
    "\n",
    "### pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "### punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spellchecker import SpellChecker\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import scattertext as st\n",
    "import re\n",
    "\n",
    "### textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "### countvectorizer, tfidfvectorizer, embeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel, Trainer, TrainingArguments, pipeline\n",
    "import umap\n",
    "\n",
    "\n",
    "### tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "### gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "### PCA\n",
    "import random\n",
    "from adjustText import adjust_text\n",
    "\n",
    "### plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### kMeans and silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "### ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04ae0e1-cf68-45bd-a7df-bf1d9cc2f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir = Path(r\"C:\\Users\\Hyemi\\Python\\TopicModeling\\Data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "file_path = data_dir / \"articles_tokenize.csv\"\n",
    "articles_full = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42045d16-1b62-4f22-bbc9-81c6eef3b0e8",
   "metadata": {},
   "source": [
    "## 2. Embedding: group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff54534-e9ad-4ee0-9d05-6bdf5456fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify journals into groups\n",
    "def classify_year_group(year):\n",
    "    if 2013 <= year <= 2016:\n",
    "        return \"1\"\n",
    "    elif 2017 <= year <= 2019:\n",
    "        return \"2\"\n",
    "    elif 2020 <= year <= 2021:\n",
    "        return \"3\"\n",
    "    elif 2022 <= year <= 2024:\n",
    "        return \"4\"\n",
    "    else:\n",
    "        return \"5\"\n",
    "\n",
    "articles_full[\"Year_Group\"] = articles_full[\"Year\"].apply(classify_year_group)\n",
    "\n",
    "# Load XLM-RoBERTa model and tokenizer\n",
    "model_name = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a67b2a2-2263-4560-8852-e256f217a081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch_get_embeddings(texts, model, tokenizer, batch_size=32):\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    # Process texts in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                           return_tensors=\"pt\", max_length=512)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract CLS token representations\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Combine all batch embeddings\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f3ae9-4bb5-4a22-ab29-46c19538acfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:   0%|                                                                   | 0/181 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Modify the column assignment\n",
    "df = articles_full[articles_full[\"Year_Group\"] == \"2\"]\n",
    "df[\"Embeddings\"] = list(batch_get_embeddings(\n",
    "    df[\"Abstract\"].tolist(), \n",
    "    model, \n",
    "    tokenizer\n",
    "))\n",
    "\n",
    "# Rest of the clustering code remains the same\n",
    "embeddings_matrix = np.vstack(df[\"Embeddings\"].values)\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "embeddings_2d = umap_model.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Optional: Add progress tracking\n",
    "print(\"Embedding generation complete. Shape:\", embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08542456-56b6-4333-b631-2a83719b38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap.umap_ as umap\n",
    "#umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "#embeddings_2d = umap_model.fit_transform(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e68dd8-2209-4b49-a747-550d74f90ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Within-Cluster Sum of Squares (WCSS) for different cluster sizes\n",
    "def plot_elbow_method(embeddings_matrix, max_clusters=9):\n",
    "    wcss = []  # List to store WCSS for each number of clusters\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(embeddings_matrix)\n",
    "        wcss.append(kmeans.inertia_)  # Inertia is the sum of squared distances to the closest centroid\n",
    "    \n",
    "    # Plot Elbow Graph\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(2, max_clusters + 1), wcss, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n",
    "    plt.title(\"Elbow Method for Optimal k\")\n",
    "    plt.xticks(range(1, max_clusters + 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# Run the Elbow Method function to determine the optimal number of clusters\n",
    "plot_elbow_method(embeddings_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335999c-5643-40d7-9fb3-a7b8c249b529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings_2d)\n",
    "\n",
    "# Add clustering results to the dataframe\n",
    "df[\"Cluster\"] = clusters\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], hue=df[\"Cluster\"], palette=\"tab10\", alpha=0.6)\n",
    "plt.title(\"Word Clusters Based on Abstract Embeddings\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789e3d4-7a9e-4147-96f2-805ebdf3f99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "umap_3d = umap.UMAP(n_components=3, random_state=42)\n",
    "embeddings_3d = umap_3d.fit_transform(embeddings_matrix)\n",
    "\n",
    "# Set number of clusters\n",
    "num_clusters = 4\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(embeddings_3d)  # Use 3D embeddings\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(\n",
    "    embeddings_3d[:, 0], embeddings_3d[:, 1], embeddings_3d[:, 2], \n",
    "    c=df[\"Cluster\"], cmap=\"tab10\", alpha=0.6\n",
    ")\n",
    "\n",
    "# Labels and Title\n",
    "ax.set_title(\"Word Clusters Based on Abstract Embeddings (3D)\")\n",
    "ax.set_xlabel(\"UMAP Dimension 1\")\n",
    "ax.set_ylabel(\"UMAP Dimension 2\")\n",
    "ax.set_zlabel(\"UMAP Dimension 3\")\n",
    "\n",
    "# Add legend\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730be58-5da5-471d-9801-bc6b7f5edc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(r\"C:\\Users\\Hyemi\\Python\\TopicModeling\\Data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = data_dir / \"articles_embedding_2_full.csv\"\n",
    "\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c69d4-8559-4b95-be7e-5236570081eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
