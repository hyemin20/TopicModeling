{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3911317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download libraries\n",
    "# ----------\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c53025",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' IMPORT LIBRARIES\n",
    "###'\n",
    "###'\n",
    "\n",
    "### pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import spacy\n",
    "\n",
    "### punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spellchecker import SpellChecker\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import scattertext as st\n",
    "\n",
    "### textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "### countvectorizer, tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "### tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "### gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "### PCA\n",
    "import random\n",
    "from adjustText import adjust_text\n",
    "\n",
    "### plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### kMeans and silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "### ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "###time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ba90f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\OWNER\\\\TopicModeling'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf4b48",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8d4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# ----------\n",
    "\n",
    "#f = pd.read_csv(\"articles_data.csv\")\n",
    "#f.head()\n",
    "#len(df)\n",
    "\n",
    "df_14 = pd.read_csv(\"articles_data_14.csv\")\n",
    "df_24 = pd.read_csv(\"articles_data_24.csv\")\n",
    "    \n",
    "df_14['Published'] = 0  # Code 0 for data from df_14\n",
    "df_24['Published'] = 1  # Code 1 for data from df_24\n",
    "\n",
    "df_14 = df_14[df_14['Abstract'] != 'Abstract not available.']\n",
    "df_24 = df_24[df_24['Abstract'] != 'Abstract not available.']\n",
    "\n",
    "#df_a = pd.concat([df_14, df_24], ignore_index=True)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a61665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac6070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_14_S = df_14[['Title','Abstract','Published']].dropna()\n",
    "df_14_S['Abstract'] = df_14_S['Abstract'].apply(lambda x: re.sub(r'<\\d+|\\*|†>', '', re.sub(r'\\<.*?\\>', '', x)))\n",
    "\n",
    "df_24_S = df_24[['Title','Abstract','Published']].dropna()\n",
    "df_24_S['Abstract'] = df_24_S['Abstract'].apply(lambda x: re.sub(r'<\\d+|\\*|†>', '', re.sub(r'\\<.*?\\>', '', x)))\n",
    "#df_S.head()\n",
    "#len(df_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61055900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_S.iloc[204:209]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9e682",
   "metadata": {},
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "### text spell check\n",
    "df_S['Title_spell'] = df_S['Title'].map(lambda x: spell.correction(x))\n",
    "df_S['Abstract_spell'] = df_S['Abstract'].map(lambda x: spell.correction(x))\n",
    "df_S.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de3dc7",
   "metadata": {},
   "source": [
    "# 2. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f3604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc_stop(text):\n",
    "    # When text is None\n",
    "    if text is None:\n",
    "        return []\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Define additional stop words\n",
    "    stop_words = STOP_WORDS | {\"abstract\", \"available\", \"student\", \"research\", \"study\", \"impact\", \"effect\",\n",
    "                               \"result\", \"al\", \"et\", \"doi\", \"googlescholar\", \"google\", \"scholar\", \"textgoogle\", \n",
    "                               \"full\", \"crossref\", \"introduction\", \"background\", \"purpose\" \"aim\", \"objective\",\"use\",\"child\"}\n",
    "\n",
    "    # Define punctuation\n",
    "    punc = set(punctuation)\n",
    "\n",
    "    # Remove punctuation\n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "\n",
    "    # Apply NLP processing\n",
    "    doc = nlp(punc_free)\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    text_lemma = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "    # Filter tokens to remove URLs, stop words, and non-alphabetic tokens\n",
    "    filtered_tokens = [word for word in text_lemma.split() if word not in stop_words and word.isalpha()]\n",
    "\n",
    "    # Return filtered tokens for TfidfVectorizer\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1875cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_14_S['Title_tokens'] = df_14_S['Title'].map(lambda x: rem_punc_stop(x))\n",
    "df_14_S['Title_join'] = df_14_S['Title_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "df_14_S['Abstract_tokens'] = df_14_S['Abstract'].map(lambda x: rem_punc_stop(x))\n",
    "df_14_S['Abstract_join'] = df_14_S['Abstract_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "\n",
    "df_24_S['Title_tokens'] = df_14_S['Title'].map(lambda x: rem_punc_stop(x))\n",
    "df_24_S['Title_join'] = df_14_S['Title_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "df_24_S['Abstract_tokens'] = df_14_S['Abstract'].map(lambda x: rem_punc_stop(x))\n",
    "df_14_S['Abstract_join'] = df_14_S['Abstract_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3486d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' Apply the Function and Tokenize Text Column\n",
    "###'\n",
    "###'\n",
    "\n",
    "### sample from the whole dataset\n",
    "df_S['Title_tokens'] = df_S['Title'].map(lambda x: rem_punc_stop(x))\n",
    "df_S['Title_join'] = df_S['Title_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "\n",
    "df_S['Abstract_tokens'] = df_S['Abstract'].map(lambda x: rem_punc_stop(x))\n",
    "df_S['Abstract_join'] = df_S['Abstract_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "\n",
    "\n",
    "#df_S['Full_tokens'] = df_S['Full Text'].map(lambda x: rem_punc_stop(x))\n",
    "#df_S['Full_join'] = df_S['Full_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\")\n",
    "\n",
    "\n",
    "df_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6dc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_S.iloc[204:209]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c556d1",
   "metadata": {},
   "source": [
    "# 3. Word Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78920e62",
   "metadata": {},
   "source": [
    "# 3.1. Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf99b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' GENERATE TEXT FEATURES\n",
    "###' e.g. text_len, word count, polarity, subjectivity\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "# text_len & count\n",
    "df_S['Title_count'] = df_S['Title'].dropna().apply(lambda x: len(str(x).split()))\n",
    "df_S['Abstract_count'] = df_S['Abstract'].dropna().apply(lambda x: len(str(x).split()))\n",
    "#df_S['Full_count'] = df_S['Full Text'].dropna().apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ae3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_S,            # specify data\n",
    "            x=\"Title_count\") # x-axis feature\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05309c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_S,              # specify data\n",
    "            x=\"Abstract_count\") # x-axis feature\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.displot(df_S,              # specify data\n",
    "#            x=\"Full_count\") # x-axis feature\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91091144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_S = df_S[df_S['Abstract_count'] >= 2000]\n",
    "#filtered_df = df_S[df_S['Abstract_count'] < 20]\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#filtered_df\n",
    "#len(df_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fded48",
   "metadata": {},
   "source": [
    "## 3.2. WordFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT with TItle\n",
    "###' _ unigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent unigrams \n",
    "countvec = CountVectorizer(min_df = 5, ngram_range=(1,1))\n",
    "ngrams = countvec.fit_transform(df_S['Title_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7051723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT with Abstract\n",
    "###' _ unigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent unigrams \n",
    "countvec = CountVectorizer(min_df = 5, ngram_range=(1,1))\n",
    "ngrams = countvec.fit_transform(df_S['Abstract_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26185cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT with Full_text\n",
    "###' _ unigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent unigrams \n",
    "countvec = CountVectorizer(min_df = 5, ngram_range=(1,1))\n",
    "ngrams = countvec.fit_transform(df_S['Full_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ca568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT\n",
    "###' _ bigrams, trigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent bigrams \n",
    "countvec = CountVectorizer(min_df = 3, ngram_range=(2,3))\n",
    "\n",
    "\n",
    "### fit and transform on tokens\n",
    "ngrams = countvec.fit_transform(df_S['Title_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824c36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT\n",
    "###' _ bigrams, trigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent bigrams \n",
    "countvec = CountVectorizer(min_df = 5, ngram_range=(2,3))\n",
    "\n",
    "\n",
    "### fit and transform on tokens\n",
    "ngrams = countvec.fit_transform(df_S['Abstract_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c76678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT\n",
    "###' _ bigrams, trigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent bigrams \n",
    "countvec = CountVectorizer(min_df = 5, ngram_range=(2,3))\n",
    "\n",
    "\n",
    "### fit and transform on tokens\n",
    "ngrams = countvec.fit_transform(df_S['Full_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT\n",
    "###' _ bigrams, trigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent bigrams \n",
    "countvec = CountVectorizer(min_df = 3, ngram_range=(3,4))\n",
    "\n",
    "\n",
    "### fit and transform on tokens\n",
    "ngrams = countvec.fit_transform(df_S['Abstract_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : WORDS COUNT\n",
    "###' _ bigrams, trigrams\n",
    "###'\n",
    "###'\n",
    "\n",
    "\n",
    "### most frequent bigrams \n",
    "countvec = CountVectorizer(min_df = 5, ngram_range=(3,4))\n",
    "\n",
    "\n",
    "### fit and transform on tokens\n",
    "ngrams = countvec.fit_transform(df_S['Full_join'])      \n",
    "\n",
    "\n",
    "### create a dataframe \n",
    "dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                    columns = countvec.get_feature_names_out()) \n",
    "\n",
    "\n",
    "# Sum and organize ngram frequencies\n",
    "df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### feature names\n",
    "feature_names = df_ngram.head(10)['ngrams'].values\n",
    "print(feature_names)\n",
    "\n",
    "### plot \n",
    "sns.barplot(x=\"ngrams\", \n",
    "            y = \"freq\",\n",
    "            data=df_ngram[0:25])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1d34f",
   "metadata": {},
   "source": [
    "# 더 넓은 period 설정해서 추출한 논문 random 추출해서 3가지 비교\n",
    "\n",
    "## frequency 추출은 2단어가 제일 나은듯 함\n",
    "## title이 제일 별로인거 같고 < 초록 < ? 전문 인듯?\n",
    "\n",
    "## 한데 topic modeling 끝까지 해봐야 할듯\n",
    "\n",
    "## keyword로 clustering하고 각 클러스터별 topic modeling?\n",
    "\n",
    "## topic modeling, 키워드, clustering 차이 확인\n",
    "## ? 까먹음\n",
    "\n",
    "### 아닌듯 키워드는 초록으로, topic modeling은 전문으로?\n",
    "\n",
    "연구문제\n",
    "\n",
    "키워드 추출하는 방법\n",
    "1. 각 논문 별 word frequncy top N개 추출 -> word embadding -> PCA\n",
    "\n",
    "- unsupervised\n",
    "2. title/abstract/fulltext 바탕으로 elbow K보고 clustering 나눠서 -> 클러스터별로 title/abstract/fulltext 바탕으로 topic modeling 결과 + 출판 년도 확인  \n",
    "3. overlab되는 부분이 있을 때는 어떤 clustering 방법이 최선???  spectral / hierarchical 찾아보기, clustering에도 앙상블 있나?\n",
    "4. 학회지 / 년도 나눠서 cluster 결과 확인ㄱㄴ\n",
    "\n",
    "- supervised\n",
    "4. 기간 나눠서 -> 기간 별 topic modeling 결과 확인\n",
    "4. 기간 나눠서 -> 기간을 잘 구별하는 topic이 뭔지 neuralnetwork/ensemble 돌리기?\n",
    "\n",
    "\n",
    "open source에는 word2vec 써서 단어-단어=? 보여주는 창 있어도 ㄱㅊ을듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8106ce4",
   "metadata": {},
   "source": [
    "## 3.2. Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : Word Cloud\n",
    "###' by. party\n",
    "###'\n",
    "###'\n",
    "\n",
    "### 0. Sample from Whole data\n",
    "\n",
    "# apply function to text object\n",
    "TO_text = ' '.join(df_S['Title_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\"))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "TO_wordcloud = WordCloud(background_color = \"white\",\n",
    "                      random_state=41).generate(TO_text)          \n",
    "\n",
    "# plot \n",
    "plt.imshow(TO_wordcloud,\n",
    "           interpolation = 'bilinear')\n",
    "plt.axis('off')                       \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : Word Cloud\n",
    "###' by. party\n",
    "###'\n",
    "###'\n",
    "\n",
    "### 0. Sample from Whole data\n",
    "\n",
    "# apply function to text object\n",
    "TO_text = ' '.join(df_S['Abstract_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\"))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "TO_wordcloud = WordCloud(background_color = \"white\",\n",
    "                      random_state=41).generate(TO_text)          \n",
    "\n",
    "# plot \n",
    "plt.imshow(TO_wordcloud,\n",
    "           interpolation = 'bilinear')\n",
    "plt.axis('off')                       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###' ################################################################################\n",
    "###'\n",
    "###' VISUALIZATION : Word Cloud\n",
    "###' by. party\n",
    "###'\n",
    "###'\n",
    "\n",
    "### 0. Sample from Whole data\n",
    "\n",
    "# apply function to text object\n",
    "TO_text = ' '.join(df_S['Full_tokens'].map(lambda text: ' '.join(text) if isinstance(text, list) else \"\"))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object \n",
    "TO_wordcloud = WordCloud(background_color = \"white\",\n",
    "                      random_state=41).generate(TO_text)          \n",
    "\n",
    "# plot \n",
    "plt.imshow(TO_wordcloud,\n",
    "           interpolation = 'bilinear')\n",
    "plt.axis('off')                       \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23edab",
   "metadata": {},
   "source": [
    "# 3.3. KeyWords with WordsFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546523c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_freq = []\n",
    "\n",
    "for _, row in df_S.iterrows():\n",
    "   \n",
    "    countvec = CountVectorizer(min_df=1, ngram_range=(2, 3))\n",
    "    ngrams = countvec.fit_transform([row['Full_join']])\n",
    "    \n",
    "    dictionary_dataframe = pd.DataFrame(ngrams.todense(),\n",
    "                                        columns = countvec.get_feature_names_out()) \n",
    "    df_ngram = pd.DataFrame(dictionary_dataframe.sum().reset_index()).rename(columns={'index': 'ngrams', 0: 'freq'})\n",
    "    df_ngram = df_ngram.sort_values(by=['freq'], ascending=False).reset_index(drop=True)\n",
    "    feature_names = \", \".join(df_ngram['ngrams'].values[:10] if len(df_ngram) >= 10 else df_ngram['ngrams'].values)\n",
    "    key_freq.append(feature_names)\n",
    "\n",
    "    \n",
    "KEY1 = pd.DataFrame(key_freq, columns=['key_freq'])\n",
    "KEY_F1 = df_S.join(KEY1)\n",
    "\n",
    "# 결과 확인\n",
    "KEY_F1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51354dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_docs = df_S.head(5)\n",
    "top_5_freq_counts = []\n",
    "\n",
    "for idx, row in top_5_docs.iterrows():\n",
    "    # 해당 문서에서 추출된 최대 10개의 n-gram을 가져옴 (쉼표로 구분된 문자열이므로 분리)\n",
    "    ngram_list = key_freq[idx].split(\", \")\n",
    "    \n",
    "    # CountVectorizer에 vocabulary로 전달할 n-gram 리스트 설정\n",
    "    countvec = CountVectorizer(ngram_range=(2, 3), vocabulary=ngram_list)\n",
    "    \n",
    "    # 해당 문서에서 n-gram 빈도수 계산\n",
    "    ngrams = countvec.fit_transform([row['Full_join']])\n",
    "    \n",
    "    # 빈도수를 DataFrame으로 생성\n",
    "    ngram_counts = pd.DataFrame(ngrams.toarray(), columns=countvec.get_feature_names_out())\n",
    "    ngram_counts['document'] = idx  # 문서 인덱스 추가\n",
    "    top_5_freq_counts.append(ngram_counts)\n",
    "\n",
    "# 모든 문서의 빈도수를 하나의 DataFrame으로 결합\n",
    "freq_counts_df = pd.concat(top_5_freq_counts).reset_index(drop=True)\n",
    "\n",
    "# 결과 확인\n",
    "freq_counts_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
